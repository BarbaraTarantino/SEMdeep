[{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Mario Grassi. Author. Barbara Tarantino. Maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Grassi M (2025). SEMdeep: Structural Equation Modeling Deep Neural Network Machine Learning algorithms. doi:10.32614/CRAN.package.SEMdeep, R package version 1.1.0, https://CRAN.R-project.org/package=SEMdeep.","code":"@Manual{,   title = {SEMdeep: Structural Equation Modeling with Deep Neural Network and Machine Learning algorithms},   author = {Mario Grassi},   year = {2025},   note = {R package version 1.1.0},   url = {https://CRAN.R-project.org/package=SEMdeep},   doi = {10.32614/CRAN.package.SEMdeep}, }"},{"path":"/index.html","id":"semdeep","dir":"","previous_headings":"","what":"Structural Equation Modeling with Deep Neural Network and Machine Learning algorithms","title":"Structural Equation Modeling with Deep Neural Network and Machine Learning algorithms","text":"Structural Equation Modeling Deep Neural Network Machine Learning SEMdeep train validate custom (data-driven) structural equation model (SEM) using deep neural networks (DNNs) machine learning (ML) algorithms. SEMdeep comes following functionalities: Automated DNN ML model training based SEM network structures. Network plot representation interpretation diagram. Model performance evaluation regression classification metrics. Compute model variable importance DNN (connection weights, gradient weights, significance tests network inputs) ML (variable importance measures, Shapley (R2) values, LOCO values).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Structural Equation Modeling with Deep Neural Network and Machine Learning algorithms","text":"SEMdeep uses deep learning framework ‘torch’. torch package native R, ’s computationally efficient, need install Python API, DNNs can trained CPU, GPU MacOS GPUs. using ‘SEMdeep’ make sure current version ‘torch’ installed running: windows (Linux Mac). Windows distributions don’t Visual C++ runtime pre-installed, download Microsoft VC_redist.x86.exe (R32) VC_redist.x86.exe (R64) install . GPU setup, problems installing torch package, check installation help torch developer. , latest stable version can installed CRAN: latest development version can installed GitHub:","code":"install.packages(\"torch\")  library(torch)  install_torch(reinstall = TRUE) install.packages(\"SEMdeep\") # install.packages(\"devtools\") devtools::install_github(\"BarbaraTarantino/SEMdeep\")"},{"path":"/index.html","id":"getting-help","dir":"","previous_headings":"","what":"Getting help","title":"Structural Equation Modeling with Deep Neural Network and Machine Learning algorithms","text":"full list SEMdeep functions examples available website .","code":""},{"path":"/reference/classificationReport.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction evaluation report of a classification model — classificationReport","title":"Prediction evaluation report of a classification model — classificationReport","text":"function builds report showing main classification metrics. provides overview key evaluation metrics like precision, recall, F1-score, accuracy, Matthew's correlation coefficient (mcc) support (testing size) class dataset averages (macro weighted) classes.","code":""},{"path":"/reference/classificationReport.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction evaluation report of a classification model — classificationReport","text":"","code":"classificationReport(yobs, yhat, CM = NULL, verbose = FALSE, ...)"},{"path":"/reference/classificationReport.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction evaluation report of a classification model — classificationReport","text":"yobs vector true target variable values. yhat matrix predicted target variables values. CM optional (external) confusion matrix CxC. verbose logical value (default = FALSE). TRUE, confusion matrix printed screen, C=2, density plots predicted probability group also printed. ... Currently ignored.","code":""},{"path":"/reference/classificationReport.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction evaluation report of a classification model — classificationReport","text":"list 3 objects: \"CM\", confusion matrix observed predicted counts. \"stats\", data.frame classification evaluation statistics. \"cls\", data.frame predicted probabilities, predicted labels true labels categorical target variable.","code":""},{"path":"/reference/classificationReport.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prediction evaluation report of a classification model — classificationReport","text":"Given one vector true target variable labels, matrix predicted target variable values class, series classification metrics computed. example, suppose 2x2 table notation formulas used label = \"Yes Event\" : $$pre = /(+B)$$ $$rec = /(+C)$$ $$F1 = (2*pre*rec)/(pre+rec)$$ $$acc = (+D)/(+B+C+D)$$ $$mcc = (*D-B*C)/sqrt((+B)*(C+D)*(+C)*(B+D))$$ Metrics analogous described calculated label \"Event\", weighted average (averaging support-weighted mean per label) macro average (averaging unweighted mean per label) also provided.","code":""},{"path":"/reference/classificationReport.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prediction evaluation report of a classification model — classificationReport","text":"Sammut, C. & Webb, G. . (eds.) (2017). Encyclopedia Machine Learning Data Mining. New York: Springer. ISBN: 978-1-4899-7685-7","code":""},{"path":"/reference/classificationReport.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prediction evaluation report of a classification model — classificationReport","text":"Barbara Tarantino barbara.tarantino@unipv.","code":""},{"path":"/reference/classificationReport.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prediction evaluation report of a classification model — classificationReport","text":"","code":"# \\donttest{ # Load Sachs data (pkc) ig<- sachs$graph data<- sachs$pkc data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- sachs$group  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  #...with a categorical (as.factor) variable (C=2) outcome<- factor(ifelse(group == 0, \"control\", \"case\")) res<- SEMml(ig, data[train, ], outcome[train], algo=\"rf\") #> DAG conversion : TRUE #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 11 iterations #>  #>  logL:-28.264333  srmr:0.064595 pred<- predict(res, data[-train, ], outcome[-train], verbose=TRUE) #>       amse         r2       srmr  #> 0.69813257 0.30186743 0.06919075   yobs<- outcome[-train] yhat<- pred$Yhat[ ,levels(outcome)] cls<- classificationReport(yobs, yhat) cls$CM #>          pred #> yobs      case control #>   case     426      36 #>   control   40     381 cls$stats #>              precision    recall        f1  accuracy      mcc support #> case         0.9141631 0.9220779 0.9181034 0.9139298 0.827449     462 #> control      0.9136691 0.9049881 0.9093079 0.9139298 0.827449     421 #> macro avg    0.9139161 0.9135330 0.9137057 0.9139298 0.827449     883 #> weighted avg 0.9139275 0.9139298 0.9139099 0.9139298 0.827449     883 #>              support_prop #> case            0.5232163 #> control         0.4767837 #> macro avg       1.0000000 #> weighted avg    1.0000000 head(cls$cls) #>        case   control    pred    yobs #> 1 0.4530052 0.5469948 control control #> 2 0.3715234 0.6284766 control control #> 3 0.2788011 0.7211989 control control #> 4 0.4437142 0.5562858 control control #> 5 0.3386615 0.6613385 control control #> 6 0.2799941 0.7200059 control control  #...with predicted probabiliy density plots, if C=2 cls<- classificationReport(yobs, yhat, verbose=TRUE) #>          pred #> yobs      case control #>   case     426      36 #>   control   40     381 #>    #...with a categorical (as.factor) variable (C=3) group[1:400]<- 2; table(group) #> group #>   0   1   2  #> 453 913 400  outcome<- factor(ifelse(group == 0, \"control\",         ifelse(group == 1, \"case1\", \"case2\"))) res<- SEMml(ig, data[train, ], outcome[train], algo=\"rf\") #> DAG conversion : TRUE #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 12 iterations #>  #>  logL:-31.155319  srmr:0.077561 pred<- predict(res, data[-train, ], outcome[-train], verbose=TRUE) #>       amse         r2       srmr  #> 0.74211577 0.25788423 0.08338864   yobs<- outcome[-train] yhat<- pred$Yhat[ ,levels(outcome)] cls<- classificationReport(yobs, yhat) cls$CM #>          pred #> yobs      case1 case2 control #>   case1     414    18      30 #>   case2      14   109      85 #>   control    16    93     104 cls$stats #>              precision    recall        f1  accuracy       mcc support #> case1        0.9324324 0.8961039 0.9139073 0.7100793 0.5314694     462 #> case2        0.4954545 0.5240385 0.5093458 0.7100793 0.5314694     208 #> control      0.4748858 0.4882629 0.4814815 0.7100793 0.5314694     213 #> macro avg    0.6342576 0.6361351 0.6349115 0.7100793 0.5314694     883 #> weighted avg 0.7191269 0.7100793 0.7142974 0.7100793 0.5314694     883 #>              support_prop #> case1           0.5232163 #> case2           0.2355606 #> control         0.2412231 #> macro avg       1.0000000 #> weighted avg    1.0000000 head(cls$cls) #>       case1     case2   control    pred  yobs #> 1 0.3013243 0.3147069 0.3839688 control case2 #> 2 0.2550408 0.2985409 0.4464183 control case2 #> 3 0.1743354 0.4132468 0.4124178   case2 case2 #> 4 0.3040700 0.3518330 0.3440970   case2 case2 #> 5 0.2269532 0.3563240 0.4167228 control case2 #> 6 0.1831267 0.4307936 0.3860797   case2 case2 # }"},{"path":"/reference/crossValidation.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"function R-repeated K-fold cross-validation SEMrun(), SEMml() SEMdnn() models.","code":""},{"path":"/reference/crossValidation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"","code":"crossValidation(   models,   outcome = NULL,   K = 5,   R = 1,   metric = NULL,   ncores = 2,   verbose = FALSE,   ... )"},{"path":"/reference/crossValidation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"models named list model fitting objects SEMrun(), SEMml() SEMdnn() function, default group=NULL (SEMrun() outcome=NULL (SEMml() SEMdnn()). outcome character vector (.factor) labels categorical output (target). NULL (default), categorical output (target) considered. K numerical value indicating number k-fold create. R numerical value indicating number repetitions k-fold cross-validation. metric character value indicating metric boxplots display, .e.: \"amse\", \"r2\", \"srmr\", continuous outcomes, \"f1\", \"accuracy\" \"mcc\", categorical outcome (default = NULL). ncores Number cpu cores (default = 2). verbose Output console boxplots summarized results (default = FALSE). ... Currently ignored.","code":""},{"path":"/reference/crossValidation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"list 2 objects: (1) \"stats\", list performance evaluation metrics. outcome=FALSE, mean (0.025;0.0975)-quantiles amse, r2, srmr across folds repetitions reported; outcome=TRUE, mean (0.025;0.0975)-quantiles f1, accuracy mcc confusion matrix averaged across repetitions reported; (2) \"PE\", data.frame repeated cross-validation results.","code":""},{"path":"/reference/crossValidation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"Easy--use model comparison selection SEM, ML DNN models, several models defined compared R-repeated K-fold cross-validation procedure. winner model selected reporting mean predicted performances across runs, outline de Rooij & Weeda (2020).","code":""},{"path":"/reference/crossValidation.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"de Rooij M, Weeda W. Cross-Validation: Method Every Psychologist Know. Advances Methods Practices Psychological Science. 2020;3(2):248-263. doi:10.1177/2515245919898466","code":""},{"path":"/reference/crossValidation.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/crossValidation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation of linear SEM, ML or DNN training models — crossValidation","text":"","code":"# \\donttest{ # Load Amyotrophic Lateral Sclerosis (ALS) ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- alsData$group  # ... with continuous outcomes   res1 <- SEMml(ig, data, algo=\"tree\") #> Running SEM model via ML... #>  done. #>  #> TREE solver ended normally after 23 iterations #>  #>  logL:-52.850417  srmr:0.196585 res2 <- SEMml(ig, data, algo=\"rf\") #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 23 iterations #>  #>  logL:-41.009436  srmr:0.0905 res3 <- SEMml(ig, data, algo=\"xgb\") #> Running SEM model via ML... #>  done. #>  #> XGB solver ended normally after 23 iterations #>  #>  logL:29.12011  srmr:0.006995 res4 <- SEMml(ig, data, algo=\"sem\") #> Running SEM model via ML... #>  done. #>  #> SEM solver ended normally after 23 iterations #>  #>  logL:-56.348113  srmr:0.288874  models <- list(res1,res2,res3,res4) names(models) <- c(\"tree\",\"rf\",\"xgb\",\"sem\")  res.cv1 <- crossValidation(models, outcome=NULL, K=5, R=10) #> Running Cross-validation... #> r-repeat = 1  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 2  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 3  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 4  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 5  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 6  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 7  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 8  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 9  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 10  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. print(res.cv1$stats) #> $amse #>      Wins  2.5%  mean 97.5% #> tree    0 0.884 0.884 0.884 #> rf      0 0.866 0.866 0.866 #> xgb     0 0.949 0.949 0.949 #> sem    10 0.803 0.803 0.803 #>  #> $r2 #>      Wins  2.5%  mean 97.5% #> tree    0 0.116 0.116 0.116 #> rf      0 0.134 0.134 0.134 #> xgb     0 0.051 0.051 0.051 #> sem    10 0.197 0.197 0.197 #>  #> $srmr #>      Wins  2.5%  mean 97.5% #> tree    0 0.253 0.253 0.253 #> rf      0 0.192 0.192 0.192 #> xgb    10 0.167 0.167 0.167 #> sem     0 0.321 0.321 0.321 #>   #... with a categorical (as.factor) outcome  outcome <- factor(ifelse(group == 0, \"control\", \"case\")) res.cv2 <- crossValidation(models, outcome=outcome, K=5, R=10) #> Running Cross-validation... #> r-repeat = 1  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 2  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 3  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 4  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 5  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 6  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 7  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 8  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 9  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. #> r-repeat = 10  #>  5-fold tree done. #>  5-fold rf done. #>  5-fold xgb done. #>  5-fold sem done. print(res.cv2$stats) #> $f1 #>      Wins  2.5%  mean 97.5% #> tree    0 0.863 0.863 0.863 #> rf      0 0.824 0.824 0.824 #> xgb    10 0.882 0.882 0.882 #> sem     0 0.759 0.759 0.759 #>  #> $accuracy #>      Wins  2.5%  mean 97.5% #> tree    0 0.850 0.850 0.850 #> rf      0 0.794 0.794 0.794 #> xgb    10 0.869 0.869 0.869 #> sem     0 0.713 0.713 0.713 #>  #> $mcc #>      Wins  2.5%  mean 97.5% #> tree    0 0.488 0.488 0.488 #> rf      0 0.497 0.497 0.497 #> xgb    10 0.588 0.588 0.588 #> sem     0 0.407 0.407 0.407 #>  # }"},{"path":"/reference/getConnectionWeight.html","id":null,"dir":"Reference","previous_headings":"","what":"Connection Weight method for neural network variable importance — getConnectionWeight","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"function computes matrix multiplications hidden weight matrices (Wx,...,Wy), .e., product raw input-hidden hidden-output connection weights input output neuron sums products across hidden neurons, proposed Olden (2002; 2004).","code":""},{"path":"/reference/getConnectionWeight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"","code":"getConnectionWeight(object, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getConnectionWeight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"object neural network object SEMdnn() function. thr numeric value [0-1] indicating threshold apply Olden's connection weights color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(connection weights)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getConnectionWeight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"list three object: () est: data.frame including connections together connection weights(W), (ii) gest: outcome vector given, data.frame connection weights outcome lavels, (iii) dag: DAG colored edges/nodes. abs(W) > thr W < 0, edge W > 0, edge activated highlighted red. outcome vector given, nodes absolute connection weights summed outcome levels, .e. sum(abs(W[outcome levels])) > thr, highlighted pink.","code":""},{"path":"/reference/getConnectionWeight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"neural network, connections inputs outputs represented connection weights neurons. importance values assigned input variable using Olden method units based directly summed product connection weights. amount direction link weights largely determine proportional contributions input variables neural network's prediction output. Input variables larger connection weights indicate higher intensities signal transfer therefore important prediction process. Positive connection weights represent excitatory effects neurons (raising intensity incoming signal) increase value predicted response, negative connection weights represent inhibitory effects neurons (reducing intensity incoming signal). weights change sign (e.g., positive negative) input-hidden hidden-output layers cancelling effect, vice versa weights sign synergistic effect. Note order map connection weights DAG edges, element-wise product, W*performed Olden's weights entered matrix, W(pxp) binary (1,0) adjacency matrix, (pxp) input DAG.","code":""},{"path":"/reference/getConnectionWeight.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"Olden, Julian & Jackson, Donald. (2002). Illuminating \"black box\": randomization approach understanding variable contributions artificial neural networks. Ecological Modelling, 154(1-2): 135-150. https://doi.org/10.1016/S0304-3800(02)00064-9 Olden, Julian; Joy, Michael K; Death, Russell G (2004). accurate comparison methods quantifying variable importance artificial neural networks using simulated data. Ecological Modelling, 178 (3-4): 389-397. https://doi.org/10.1016/j.ecolmodel.2004.03.013","code":""},{"path":"/reference/getConnectionWeight.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getConnectionWeight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connection Weight method for neural network variable importance — getConnectionWeight","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # Load Sachs data (pkc) ig<- sachs$graph data<- sachs$pkc data<- log(data)  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data)) #ncores<- parallel::detectCores(logical = FALSE)  dnn0<- SEMdnn(ig, data[train, ], outcome = NULL, algo= \"structured\",       hidden = c(10,10,10), link = \"selu\", bias = TRUE,       epochs = 32, patience = 10, verbose = TRUE)  cw<- getConnectionWeight(dnn0, thr = 0.3, verbose = FALSE) gplot(cw$dag, l=\"circo\") table(E(cw$dag)$color) } #> DAG conversion : TRUE #> Running SEM model via DNN... #> Warning: Mask check failed! Run a new DNN with a single width hidden layer #> Loss at epoch 10: 0.901556, l1: 0.00000 #> Loss at epoch 20: 0.859576, l1: 0.00000 #> Loss at epoch 30: 0.851566, l1: 0.00000 #>  done. #>  #> DNN solver ended normally after 32 iterations #>  #>  logL:-29.50684  srmr:0.338346  #>  #> gray50   red2  #>     14      4  # }"},{"path":"/reference/getGradientWeight.html","id":null,"dir":"Reference","previous_headings":"","what":"Gradient Weight method for neural network variable importance — getGradientWeight","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"function computes gradient matrix, .e., average marginal effect input variables w.r.t neural network model, discussed Scholbeck et al (2024).","code":""},{"path":"/reference/getGradientWeight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"","code":"getGradientWeight(object, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getGradientWeight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"object neural network object SEMdnn() function. thr numeric value [0-1] indicating threshold apply gradient weights color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(gradient weights)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getGradientWeight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"list three object: () est: data.frame including connections together gradient weights, (ii) gest: outcome vector given, data.frame gradient weights outcome lavels, (iii) dag: DAG colored edges/nodes. abs(grad) > thr grad < 0, edge inhibited highlighted blue; otherwise, abs(grad) > thr grad > 0, edge activated highlighted red. outcome vector given, nodes absolute connection weights summed outcome levels, .e. sum(abs(grad[outcome levels])) > thr, highlighted pink.","code":""},{"path":"/reference/getGradientWeight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"gradient weights method approximate derivative (gradient) output variable (y) respect input variable (x) evaluated observation (=1,...,n) training data. contribution input evaluated terms magnitude taking account connection weights activation functions, also values observation input variables. gradients variable observation, summary gradient calculated averaging observation units. Finally, average weights entered matrix, W(pxp) element-wise product binary (1,0) adjacency matrix, (pxp) input DAG, W*maps weights DAG edges. Note operations required approsimate partial derivatives time consuming compared methods Olden's (connection weight). computational time increases size neural network size data.","code":""},{"path":"/reference/getGradientWeight.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"Scholbeck, C.., Casalicchio, G., Molnar, C. et al. Marginal effects non-linear prediction functions. Data Min Knowl Disc 38, 2997–3042 (2024). https://doi.org/10.1007/s10618-023-00993-x","code":""},{"path":"/reference/getGradientWeight.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getGradientWeight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gradient Weight method for neural network variable importance — getGradientWeight","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # Load Sachs data (pkc) ig<- sachs$graph data<- sachs$pkc data<- log(data)  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data)) #ncores<- parallel::detectCores(logical = FALSE)  dnn0<- SEMdnn(ig, data[train, ], outcome = NULL, algo= \"neuralgraph\",       hidden = c(10,10,10), link = \"selu\", bias = TRUE,       epochs = 32, patience = 10, verbose = TRUE)  gw<- getGradientWeight(dnn0, thr = 0.3, verbose = FALSE) gplot(gw$dag, l=\"circo\") table(E(gw$dag)$color) } #> DAG conversion : TRUE #> Running SEM model via DNN... #> Loss at epoch 10: 0.237081, l1: 0.19671 #> Loss at epoch 20: 0.139642, l1: 0.13101 #> Loss at epoch 30: 0.088302, l1: 0.09489 #>  done. #>  #> DNN solver ended normally after 32 iterations #>  #>  logL:-19.646363  srmr:0.06331  #>  #>     gray50       red2 royalblue3  #>         14          3          1  # }"},{"path":"/reference/getLOCO.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute variable importance using LOCO values — getLOCO","title":"Compute variable importance using LOCO values — getLOCO","text":"function computes contributions variable individual predictions using LOCO (Leave COvariates) values.","code":""},{"path":"/reference/getLOCO.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute variable importance using LOCO values — getLOCO","text":"","code":"getLOCO(   object,   newdata,   newoutcome = NULL,   thr = NULL,   ncores = 2,   verbose = FALSE,   ... )"},{"path":"/reference/getLOCO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute variable importance using LOCO values — getLOCO","text":"object model fitting object SEMml(), SEMrun() functions. newdata matrix containing new data rows corresponding subjects, columns variables. newoutcome new character vector (.factor) labels categorical output (target)(default = NULL). thr numeric value [0-1] indicating threshold apply LOCO values color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(LOCO values)). ncores number cpu cores (default = 2) verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getLOCO.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute variable importance using LOCO values — getLOCO","text":"list od three object: () est: data.frame including connections together LOCO values; (iii) gest: outcome vector given, data.frame LOCO values per outcome levels; (iii) dag: DAG colored edges/nodes. LOCO > thr, edge highlighted red. outcome vector given, nodes absolute connection weights summed outcome levels, .e. sum(LOCO[outcome levels]) > thr, highlighted pink.","code":""},{"path":"/reference/getLOCO.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute variable importance using LOCO values — getLOCO","text":"LOCO (Verdinelli & Wasserman, 2024) model-agnostic method assessing importance individual features (covariates) ML predictive model. procedure simple: () train model full dataset (covariates) (ii) covariate interest: () remove (leave ) covariate dataset; (b) retrain model remaining features; (c) compare predictions full model reduced mode, (d) evaluate difference performance (e.g., using MSE, etc.). LOCO computationally expensive (requires retraining feature). getLOCO() function uses lowest computation cost procedure (see Delicando & Pena, 2023). individual relevance variable measured comparing predictions model test set obtained variable interest leave-substituted ghost variable test set. ghost variable defined linear prediction covariate using rest variables ML model. method yields similar LOCO results requires much less computing time.","code":""},{"path":"/reference/getLOCO.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute variable importance using LOCO values — getLOCO","text":"Verdinelli, ; Wasserman, L. Feature Importance: Closer Look Shapley Values LOCO. Statist. Sci. 39 (4) 623 - 636, November 2024. https://doi.org/10.1214/24-STS937 Delicado, P.; Peña, D. Understanding complex predictive models ghost variables. TEST 32, 107–145 (2023). https://doi.org/10.1007/s11749-022-00826-x","code":""},{"path":"/reference/getLOCO.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute variable importance using LOCO values — getLOCO","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getLOCO.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute variable importance using LOCO values — getLOCO","text":"","code":"# \\donttest{ # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done.  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  rf0<- SEMml(ig, data[train, ], algo=\"rf\") #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 23 iterations #>  #>  logL:-33.16687  srmr:0.086188  res<- getLOCO(rf0, data[-train, ], thr=0.2, verbose=TRUE)  table(E(res$dag)$color) #>  #> gray50   red2  #>     33     12  # }"},{"path":"/reference/getShapleyR2.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute variable importance using Shapley (R2) values — getShapleyR2","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"function computes model-agnostic variable importance based Shapley-value decomposition model R-Squared (R2, .e., coefficient determination) allocates proportion model- explained variability data model feature (Redell, 2019).","code":""},{"path":"/reference/getShapleyR2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"","code":"getShapleyR2(   object,   newdata,   newoutcome = NULL,   thr = NULL,   ncores = 2,   verbose = FALSE,   ... )"},{"path":"/reference/getShapleyR2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"object model fitting object SEMml(), SEMrun() functions. newdata matrix containing new data rows corresponding subjects, columns variables. newoutcome new character vector (.factor) labels categorical output (target)(default = NULL). thr numeric value [0-1] indicating threshold apply signed Shapley R2 color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(signed Shapley R2 values)). ncores number cpu cores (default = 2) verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getShapleyR2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"list od four object: () shapx: list individual Shapley values predictors variables per response variable; (ii) est: data.frame including connections together signed Shapley R-squred values; (iii) gest: outcome vector given, data.frame signed Shapley R-squred values per outcome levels; (iv) dag: DAG colored edges/nodes. abs(sign_r2) > thr sign_r2 < 0, edge inhibited highlighted blue; otherwise, abs(sign_r2) > thr sign_r2 > 0, edge activated highlighted red. outcome vector given, nodes absolute connection weights summed outcome levels, .e. sum(abs(sign_r2[outcome levels])) > thr, highlighted pink.","code":""},{"path":"/reference/getShapleyR2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"Shapley values (Shapley, 1953; Lundberg & Lee, 2017) apply fair distribution payoffs principles game theory measure additive contribution individual predictors ML model. function compute signed Shapley R2 metric, combines additive property Shapley values robustness R-squared (R2) Gelman (2018) produce variance decomposition accurately captures contribution variable ML model, see Redell (2019). signed values used order denote regulation connections line linear model, since edges DAG indicate node regulation (activation, positive; inhibition, negative). sign recovered edge using sign(beta), .e., sign coefficient estimates linear model (lm) fitting output node input nodes (see Joseph, 2019). Furthermore, determine local significance node regulation DAG, Shapley decomposition R-squared values outcome node (r=1,...,R) can done summing Shapley R2 indices input nodes. noted operations required compute Shapley values processed kernelshap function kernelshap R package inherently time-consuming, computational time increasing proportion number predictor variables number observations.","code":""},{"path":"/reference/getShapleyR2.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"Shapley, L. (1953) Value n-Person Games. : Kuhn, H. Tucker, ., Eds., Contributions Theory Games II, Princeton University Press, Princeton, 307-317. Scott M. Lundberg, Su-Lee. (2017). unified approach interpreting model predictions. Proceedings 31st International Conference Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 4768–4777. Redell, N. (2019). Shapley Decomposition R-Squared Machine Learning Models. arXiv preprint: https://doi.org/10.48550/arXiv.1908.09718 Gelman, ., Goodrich, B., Gabry, J., & Vehtari, . (2019). R-squared Bayesian Regression Models. American Statistician, 73(3), 307–309. Joseph, . Parametric inference universal function approximators (2019). Bank England working papers 784, Bank England, revised 22 Jul 2020.","code":""},{"path":"/reference/getShapleyR2.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getShapleyR2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"","code":"# \\donttest{ # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done.  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  rf0<- SEMml(ig, data[train, ], algo=\"rf\") #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 23 iterations #>  #>  logL:-33.16687  srmr:0.086188  res<- getShapleyR2(rf0, data[-train, ], thr=NULL, verbose=TRUE)  table(E(res$dag)$color) #>  #>     gray50       red2 royalblue3  #>         32          7          6   # shapley R2 per response variables R2<- abs(res$est[,4]) Y<- res$est[,1] R2Y<- aggregate(R2~Y,data=data.frame(R2,Y),FUN=\"sum\");R2Y #>        Y        R2 #> 1  10452 0.3074106 #> 2   1432 0.2746912 #> 3   1616 0.1025127 #> 4   4217 0.2416770 #> 5   4741 0.2133041 #> 6   4744 0.2336151 #> 7   4747 0.3015739 #> 8  54205 0.2243528 #> 9   5530 0.4424930 #> 10  5532 0.3852137 #> 11  5533 0.3368840 #> 12  5534 0.4944426 #> 13  5535 0.5919868 #> 14  5600 0.2887038 #> 15  5603 0.0000000 #> 16  5606 0.2963567 #> 17  5608 0.5054904 #> 18   596 0.2057984 #> 19  6300 0.3769140 #> 20 79139 0.4867639 #> 21   836 0.5841519 #> 22 84134 0.4718743 #> 23   842 0.3401094 r2<- mean(R2Y$R2);r2 #> [1] 0.3350574 # }"},{"path":"/reference/getSignificanceTest.html","id":null,"dir":"Reference","previous_headings":"","what":"Test for the significance of neural network input nodes — getSignificanceTest","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"function computes formal test significance neural network input nodes, based linear relationship observed output predicted values input variable, input variables maintained mean (zero) values, proposed Mohammadi (2018).","code":""},{"path":"/reference/getSignificanceTest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"","code":"getSignificanceTest(object, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getSignificanceTest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"object neural network object SEMdnn() function. thr numeric value [0-1] indicating threshold apply t-test values color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(t-test values)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getSignificanceTest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"list three object: () est: data.frame including connections together t_test weights, (ii) gest: outcome vector given, data.frame t_test weights outcome lavels, (iii) dag: DAG colored edges/nodes. abs(t_test) > thr t_test < 0, edge inhibited highlighted blue; otherwise, abs(t_test) > thr t_test > 0, edge activated highlighted red. outcome vector given, nodes absolute connection weights summed outcome levels, .e. sum(abs(t_test[outcome levels])) > thr, highlighted pink.","code":""},{"path":"/reference/getSignificanceTest.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"neural network trained, taking account number hidden layers, neurons, activation function. , network's output simulated get  predicted values output variable, fixing inputs (exception one nonconstant input variable) mean values. Subsequently, network's predictions stored process completed input variable. last step, multiple regression analysis applied node-wise (mapping input DAG) observed output nodes predicted values input nodes explanatory variables. statistical significance coefficients evaluated using standard t-student values, represent importance input variables.","code":""},{"path":"/reference/getSignificanceTest.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"S. Mohammadi. new test significance neural network inputs. Neurocomputing 2018; 273: 304-322. https://doi.org/10.1016/j.neucom.2017.08.007","code":""},{"path":"/reference/getSignificanceTest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getSignificanceTest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test for the significance of neural network input nodes — getSignificanceTest","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # Load Sachs data (pkc) ig<- sachs$graph data<- sachs$pkc data<- log(data)  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data)) #ncores<- parallel::detectCores(logical = FALSE)  dnn0<- SEMdnn(ig, data[train, ], outcome = NULL, algo= \"nodewise\",       hidden = c(10,10,10), link = \"selu\", bias = TRUE,       epochs = 32, patience = 10, verbose = TRUE)  st<- getSignificanceTest(dnn0, thr = NULL, verbose = FALSE) gplot(st$dag, l=\"circo\") table(E(st$dag)$color) } #> DAG conversion : TRUE #> Running SEM model via DNN... #>  #> node 1 : zAkt  #>     train       val      base  #> 0.5872679       Inf 0.9988675  #>  #> node 2 : zErk  #>     train       val      base  #> 0.7163617       Inf 0.9988675  #>  #> node 3 : zJnk  #>     train       val      base  #> 0.8443861       Inf 0.9988675  #>  #> node 4 : zMek  #>     train       val      base  #> 0.4878106       Inf 0.9988675  #>  #> node 5 : zP38  #>     train       val      base  #> 0.4421826       Inf 0.9988676  #>  #> node 6 : zPIP2  #>     train       val      base  #> 0.5390199       Inf 0.9988675  #>  #> node 7 : zPKC  #>     train       val      base  #> 0.9136113       Inf 0.9988676  #>  #> node 8 : zPlcg  #>     train       val      base  #> 0.8301485       Inf 0.9988675  #>  #> node 9 : zRaf  #>     train       val      base  #> 0.8999817       Inf 0.9988676  #>  done. #>  #> DNN solver ended normally after 288 iterations #>  #>  logL:-29.093807  srmr:0.257738  #>  #>     gray50       red2 royalblue3  #>         13          3          2  # }"},{"path":"/reference/getVariableImportance.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance for Machine Learning models — getVariableImportance","title":"Variable importance for Machine Learning models — getVariableImportance","text":"Extraction ML variable importance measures.","code":""},{"path":"/reference/getVariableImportance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance for Machine Learning models — getVariableImportance","text":"","code":"getVariableImportance(object, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getVariableImportance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance for Machine Learning models — getVariableImportance","text":"object model fitting object SEMml() function. thr numeric value [0-1] indicating threshold apply variable importance values color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(variable importance values)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getVariableImportance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable importance for Machine Learning models — getVariableImportance","text":"list three object: () est: data.frame including connections together variable importances (VarImp)), (ii) gest: outcome vector given, data.frame VarImp outcome lavels, (iii) dag: DAG colored edges/nodes. abs(VarImp) > thr highlighted red (VarImp > 0) blue (VarImp < 0). outcome vector given, nodes variable importances summed outcome levels, .e. sum(VarImp[outcome levels])) > thr, highlighted pink.","code":""},{"path":"/reference/getVariableImportance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Variable importance for Machine Learning models — getVariableImportance","text":"variable (predictor) importance computed considering: () absolute value z-statistic model parameters \"sem\"; (ii) variable importance measures rpart, importance xgb.importance functions \"tree\", \"rf\" \"xgb\" methods.","code":""},{"path":"/reference/getVariableImportance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Variable importance for Machine Learning models — getVariableImportance","text":"add references","code":""},{"path":"/reference/getVariableImportance.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Variable importance for Machine Learning models — getVariableImportance","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getVariableImportance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable importance for Machine Learning models — getVariableImportance","text":"","code":"# \\donttest{ # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done.  #ncores<- parallel::detectCores(logical = FALSE) ml0<- SEMml(ig, data, outcome=NULL, algo=\"rf\", ncores=2) #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 23 iterations #>  #>  logL:-41.009436  srmr:0.0905  vi05<- getVariableImportance(ml0, thr=0.5, verbose=TRUE)  table(E(vi05$dag)$color) #>  #> gray50   red2  #>     34     11  # }"},{"path":"/reference/mapGraph.html","id":null,"dir":"Reference","previous_headings":"","what":"Map additional variables (nodes) to a graph object — mapGraph","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"function insert additional nodes graph object. Among node types, additional source sink nodes can added. Source nodes can represent: () data variables; (ii) group variable; (iii) latent variables (LV). Vice versa, sink nodes represent levels categorical outcome variable linked graph nodes. Moreover, mapGraph() can also create new graph object starting compact symbolic formula.","code":""},{"path":"/reference/mapGraph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"","code":"mapGraph(graph, type, C = NULL, LV = NULL, f = NULL, verbose = FALSE, ...)"},{"path":"/reference/mapGraph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"graph igraph object. type character value specifying type mapping. Five types can specified. \"source\", source nodes linked sink nodes graph. \"group\", additional group source node added graph. \"outcome\", additional c=1,2,...,C sink nodes added graph. \"LV\", additional latent variable (LV) source nodes added graph. \"clusterLV\", series clusters data computed different LV source node added separately cluster. C number labels categorical sink node (default = NULL). LV number LV source nodes add graph. argument needs specified type = \"LV\". type = \"clusterLV\" LV number defined internally equal number clusters. (default = NULL). f formula object (default = NULL). new graph object created according specified formula object. verbose TRUE disply mapped graph (default = FALSE) ... Currently ignored.","code":""},{"path":"/reference/mapGraph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"mapGraph returns invisibly graphical object mapped node variables.","code":""},{"path":"/reference/mapGraph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/mapGraph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"","code":"# Load Amyotrophic Lateral Sclerosis (ALS) ig<- alsData$graph; gplot(ig)   # ... map source nodes to sink nodes of ALS graph  ig1 <- mapGraph(ig, type = \"source\"); gplot(ig1, l=\"dot\")   # ... map group source node to ALS graph  ig2 <- mapGraph(ig, type = \"group\"); gplot(ig2, l=\"fdp\")   # ... map outcome sink (C=2) to ALS graph  ig3 <- mapGraph(ig, type = \"outcome\", C=2); gplot(ig3, l=\"fdp\")   # ... map LV source nodes to ALS graph  ig4 <- mapGraph(ig, type = \"LV\", LV = 3); gplot(ig4, l=\"fdp\")   # ... map LV source nodes to the cluster nodes of ALS graph  ig5 <- mapGraph(ig, type = \"clusterLV\"); gplot(ig5, l=\"dot\") #> modularity = 0.5588502  #>  #> Community sizes #>  3  1  4  2  #>  4  8  9 11  #>    # ... create a new graph with the formula variables formula <- as.formula(\"z4747 ~ z1432 + z5603 + z5630\") ig6 <- mapGraph(f=formula); gplot(ig6)"},{"path":"/reference/nplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a plot for a neural network model — nplot","title":"Create a plot for a neural network model — nplot","text":"function uses plotnet function NeuralNetTools R package draw neural network plot visualize hidden layer structure.","code":""},{"path":"/reference/nplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a plot for a neural network model — nplot","text":"","code":"nplot(object, hidden, bias = TRUE, sleep = 2, ...)"},{"path":"/reference/nplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a plot for a neural network model — nplot","text":"object neural network model object hidden hidden structure object bias logical value, indicating whether draw biases layers (default = FALSE). sleep Suspend plot display specified time (secs, default = 2). ... Currently ignored.","code":""},{"path":"/reference/nplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a plot for a neural network model — nplot","text":"function invisibly returns graphical objects representing neural network architecture designed NeuralNetTools.","code":""},{"path":"/reference/nplot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a plot for a neural network model — nplot","text":"induced subgraph input graph mapped data variables. Based estimated connection weights, connection weight W > 0, connection activated highlighted red; W < 0, connection inhibited highlighted blue.","code":""},{"path":"/reference/nplot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a plot for a neural network model — nplot","text":"Beck, M.W. 2018. NeuralNetTools: Visualization Analysis Tools Neural Networks. Journal Statistical Software. 85(11):1-20.","code":""},{"path":"/reference/nplot.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create a plot for a neural network model — nplot","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/nplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a plot for a neural network model — nplot","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data  #ncores<- parallel::detectCores(logical = FALSE) dnn0 <- SEMdnn(ig, data, train=1:nrow(data), algo = \"layerwise\",       hidden = c(10, 10, 10), link = \"selu\", bias =TRUE,       epochs = 32, patience = 10, verbose = TRUE)   #Visualize the neural networks per each layer of dnn0  nplot(dnn0, hidden = c(10, 10, 10), bias = FALSE) } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> Running SEM model via DNN... #>  #> layer 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 ... #>    train      val     base  #> 0.445215      Inf 0.993750  #>  #> layer 2 : z842 z1432 z5600 z5603 z6300  #>     train       val      base  #> 0.5153647       Inf 0.9937500  #>  #> layer 3 : z54205 z5606 z5608  #>     train       val      base  #> 0.5990095       Inf 0.9937500  #>  #> layer 4 : z596 z4217  #>     train       val      base  #> 0.9540080       Inf 0.9937501  #>  #> layer 5 : z1616  #>     train       val      base  #> 0.8792974       Inf 0.9937500  #>  done. #>  #> DNN solver ended normally after 160 iterations #>  #>  logL:-50.309658  srmr:0.207363      # }"},{"path":"/reference/predict.DNN.html","id":null,"dir":"Reference","previous_headings":"","what":"SEM-based out-of-sample prediction using DNN — predict.DNN","title":"SEM-based out-of-sample prediction using DNN — predict.DNN","text":"Predict method DNN objects.","code":""},{"path":"/reference/predict.DNN.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SEM-based out-of-sample prediction using DNN — predict.DNN","text":"","code":"# S3 method for class 'DNN' predict(object, newdata, newoutcome = NULL, verbose = FALSE, ...)"},{"path":"/reference/predict.DNN.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SEM-based out-of-sample prediction using DNN — predict.DNN","text":"object model fitting object SEMdnn() function. newdata matrix containing new data rows corresponding subjects, columns variables. newdata = NULL, train data used. newoutcome new character vector (.factor) labels categorical output (target) (default = NULL). verbose Print predicted --sample MSE values (default = FALSE). ... Currently ignored.","code":""},{"path":"/reference/predict.DNN.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SEM-based out-of-sample prediction using DNN — predict.DNN","text":"list three objects: \"PE\", vector amse = average MSE (sink mediators) graph nodes; r2 = 1 - amse; srmr= Standardized Root Means Square Residual --bag correlation matrix model correlation matrix. \"mse\", vector Mean Squared Error (MSE) --bag prediction sink mediators graph nodes. \"Yhat\", matrix continuous predicted values graph nodes (excluding source nodes) based --bag samples.","code":""},{"path":"/reference/predict.DNN.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SEM-based out-of-sample prediction using DNN — predict.DNN","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/predict.DNN.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SEM-based out-of-sample prediction using DNN — predict.DNN","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # Load Amyotrophic Lateral Sclerosis (ALS) ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data group<- alsData$group   #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data)) #ncores<- parallel::detectCores(logical = FALSE)  start<- Sys.time() dnn0 <- SEMdnn(ig, data[train, ], algo =\"layerwise\",       hidden = c(10,10,10), link = \"selu\", bias = TRUE,       epochs = 32, patience = 10, verbose = TRUE) end<- Sys.time() print(end-start) pred.dnn <- predict(dnn0, data[-train, ], verbose=TRUE)  # SEMrun vs. SEMdnn MSE comparison sem0 <- SEMrun(ig, data[train, ], algo=\"ricf\", n_rep=0) pred.sem <- predict(sem0, data[-train,], verbose=TRUE)  #...with a categorical (as.factor) outcome outcome <- factor(ifelse(group == 0, \"control\", \"case\")); table(outcome)   start<- Sys.time() dnn1 <- SEMdnn(ig, data[train, ], outcome[train], algo =\"layerwise\",       hidden = c(10,10,10), link = \"selu\", bias = TRUE,       epochs = 32, patience = 10, verbose = TRUE) end<- Sys.time() print(end-start)  pred <- predict(dnn1, data[-train, ], outcome[-train], verbose=TRUE) yhat <- pred$Yhat[ ,levels(outcome)]; head(yhat) yobs <- outcome[-train]; head(yobs) classificationReport(yobs, yhat, verbose=TRUE)$stats } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> Running SEM model via DNN... #>  #> layer 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 ... #>     train       val      base  #> 0.4104707       Inf 0.9875000  #>  #> layer 2 : z842 z1432 z5600 z5603 z6300  #>     train       val      base  #> 0.4693277       Inf 0.9875000  #>  #> layer 3 : z54205 z5606 z5608  #>     train       val      base  #> 0.5376277       Inf 0.9875000  #>  #> layer 4 : z596 z4217  #>     train       val      base  #> 0.8641988       Inf 0.9875001  #>  #> layer 5 : z1616  #>     train       val      base  #> 0.8268933       Inf 0.9875001  #>  done. #>  #> DNN solver ended normally after 160 iterations #>  #>  logL:-40.969572  srmr:0.182023 #> Time difference of 15.07302 secs #>      amse        r2      srmr  #> 0.7186168 0.2813832 0.2397997  #> RICF solver ended normally after 2 iterations  #>  #> deviance/df: 6.262846  srmr: 0.3040025  #>  #>      amse        r2      srmr  #> 0.8571886 0.1428114 0.2948502  #> Running SEM model via DNN... #>  #> layer 1 : zcase zcontrol  #>       train         val        base  #> 0.003214309         Inf 0.987500012  #>  #> layer 2 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 ... #>     train       val      base  #> 0.4285718       Inf 0.9875000  #>  #> layer 3 : z842 z1432 z5600 z5603 z6300  #>     train       val      base  #> 0.4605743       Inf 0.9875000  #>  #> layer 4 : z54205 z5606 z5608  #>     train       val      base  #> 0.5376339       Inf 0.9875000  #>  #> layer 5 : z596 z4217  #>     train       val      base  #> 0.8655762       Inf 0.9875001  #>  #> layer 6 : z1616  #>     train       val      base  #> 0.8682160       Inf 0.9875001  #>  done. #>  #> DNN solver ended normally after 192 iterations #>  #>  logL:-39.856559  srmr:0.177495 #> Time difference of 27.0571 secs #>      amse        r2      srmr  #> 0.6967868 0.3032132 0.2357844  #>          pred #> yobs      case control #>   case      62      12 #>   control    1       5 #>   #>              precision    recall        f1 accuracy       mcc support #> case         0.9841270 0.8378378 0.9051095   0.8375 0.4321455      74 #> control      0.2941176 0.8333333 0.4347826   0.8375 0.4321455       6 #> macro avg    0.6391223 0.8355856 0.6699460   0.8375 0.4321455      80 #> weighted avg 0.9323763 0.8375000 0.8698350   0.8375 0.4321455      80 #>              support_prop #> case                0.925 #> control             0.075 #> macro avg           1.000 #> weighted avg        1.000 # }"},{"path":"/reference/predict.ML.html","id":null,"dir":"Reference","previous_headings":"","what":"SEM-based out-of-sample prediction using nodewise ML — predict.ML","title":"SEM-based out-of-sample prediction using nodewise ML — predict.ML","text":"Predict method ML objects.","code":""},{"path":"/reference/predict.ML.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SEM-based out-of-sample prediction using nodewise ML — predict.ML","text":"","code":"# S3 method for class 'ML' predict(object, newdata, newoutcome = NULL, ncores = 2, verbose = FALSE, ...)"},{"path":"/reference/predict.ML.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SEM-based out-of-sample prediction using nodewise ML — predict.ML","text":"object model fitting object SEMml() function. newdata matrix containing new data rows corresponding subjects, columns variables. newoutcome new character vector (.factor) labels categorical output (target)(default = NULL). ncores number cpu cores (default = 2) verbose Print predicted --sample MSE values (default = FALSE). ... Currently ignored.","code":""},{"path":"/reference/predict.ML.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SEM-based out-of-sample prediction using nodewise ML — predict.ML","text":"list 3 objects: \"PE\", vector amse = average MSE (sink mediators) graph nodes; r2 = 1 - amse; srmr= Standardized Root Means Squared Residual --bag correlation matrix model correlation matrix. \"mse\", vector Mean Squared Error (MSE) --bag prediction sink mediators graph nodes. \"Yhat\", matrix continuous predicted values graph nodes (excluding source nodes) based --bag samples.","code":""},{"path":"/reference/predict.ML.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SEM-based out-of-sample prediction using nodewise ML — predict.ML","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/predict.ML.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SEM-based out-of-sample prediction using nodewise ML — predict.ML","text":"","code":"# \\donttest{ # Load Amyotrophic Lateral Sclerosis (ALS) ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- alsData$group  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  start<- Sys.time() # ... tree res1<- SEMml(ig, data[train, ], algo=\"tree\") #> Running SEM model via ML... #>  done. #>  #> TREE solver ended normally after 23 iterations #>  #>  logL:-45.080145  srmr:0.201877 mse1<- predict(res1, data[-train, ], verbose=TRUE) #>       amse         r2       srmr  #> 0.93770362 0.06229638 0.22693793   # ... rf res2<- SEMml(ig, data[train, ], algo=\"rf\") #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 23 iterations #>  #>  logL:-33.16687  srmr:0.086188 mse2<- predict(res2, data[-train, ], verbose=TRUE) #>       amse         r2       srmr  #> 0.92435872 0.07564128 0.17600941   # ... xgb res3<- SEMml(ig, data[train, ], algo=\"xgb\") #> Running SEM model via ML... #>  done. #>  #> XGB solver ended normally after 23 iterations #>  #>  logL:70.10035  srmr:0.001439 mse3<- predict(res3, data[-train, ], verbose=TRUE) #>       amse         r2       srmr  #> 0.97398929 0.02601071 0.15433519   # ... sem res4<- SEMml(ig, data[train, ], algo=\"sem\") #> Running SEM model via ML... #>  done. #>  #> SEM solver ended normally after 23 iterations #>  #>  logL:-48.441286  srmr:0.306438 mse4<- predict(res4, data[-train, ], verbose=TRUE) #>      amse        r2      srmr  #> 0.8572186 0.1427814 0.2972509  end<- Sys.time() print(end-start) #> Time difference of 6.890379 secs  #...with a categorical (as.factor) outcome outcome <- factor(ifelse(group == 0, \"control\", \"case\")); table(outcome)  #> outcome #>    case control  #>     139      21   res5 <- SEMml(ig, data[train, ], outcome[train], algo=\"tree\") #> Running SEM model via ML... #>  done. #>  #> TREE solver ended normally after 25 iterations #>  #>  logL:-48.72171  srmr:0.196861 pred <- predict(res5, data[-train, ], outcome[-train], verbose=TRUE) #>       amse         r2       srmr  #> 0.91133182 0.08866818 0.22538176  yhat <- pred$Yhat[ ,levels(outcome)]; head(yhat) #>         case    control #> 1  0.4773726 -0.4773726 #> 2  0.4773726 -0.4773726 #> 3  0.4773726 -0.4773726 #> 4  0.4773726 -0.4773726 #> 5  0.4773726 -0.4773726 #> 6 -0.6137648  0.6137648 yobs <- outcome[-train]; head(yobs) #> [1] case case case case case case #> Levels: case control classificationReport(yobs, yhat, verbose=TRUE)$stats #>          pred #> yobs      case control #>   case      57      17 #>   control    2       4 #>   #>              precision    recall        f1 accuracy      mcc support #> case         0.9661017 0.7702703 0.8571429   0.7625 0.261562      74 #> control      0.1904762 0.6666667 0.2962963   0.7625 0.261562       6 #> macro avg    0.5782889 0.7184685 0.5767196   0.7625 0.261562      80 #> weighted avg 0.9079298 0.7625000 0.8150794   0.7625 0.261562      80 #>              support_prop #> case                0.925 #> control             0.075 #> macro avg           1.000 #> weighted avg        1.000 # }"},{"path":"/reference/predict.SEM.html","id":null,"dir":"Reference","previous_headings":"","what":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"Given values (observed) x-variables SEM, function may used predict values (observed) y-variables. predictive procedure consists two steps. First, topological layer ordering input graph defined. , node y values layer predicted, nodes successive layers act x-predictors.","code":""},{"path":"/reference/predict.SEM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"","code":"# S3 method for class 'SEM' predict(object, newdata, newoutcome = NULL, verbose = FALSE, ...)"},{"path":"/reference/predict.SEM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"object object, created function SEMrun() argument group set default group = NULL. newdata matrix new data, rows corresponding subjects, columns variables. newoutcome new character vector (.factor) labels categorical output (target)(default = NULL). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/predict.SEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"list 3 objects: \"PE\", vector amse = average MSE (sink mediators) graph nodes; r2 = 1 - amse; srmr= Standardized Root Means Square Residual --bag correlation matrix model correlation matrix. \"mse\", vector Mean Squared Error (MSE) --bag prediction sink mediators graph nodes. \"Yhat\", matrix continuous predicted values graph nodes (excluding source nodes) based --bag samples.","code":""},{"path":"/reference/predict.SEM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"function first creates layer-based structure input graph. , SEM-based predictive approach (Rooij et al., 2022) used produce predictions accounting graph structure based topological layer (j=1,…,L) input graph. iteration, response (output) variables, y nodes j=1,...,(L-1) layer predictor (input) variables, x nodes belonging successive, (j+1),...,L layers. Predictions (y given x) based (joint y x) model-implied variance-covariance (Sigma) matrix mean vector (Mu) fitted SEM, standard expression conditional mean multivariate normal distribution. Thus, layer structure described SEM taken consideration, differs ordinary least squares (OLS) regression.","code":""},{"path":"/reference/predict.SEM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"de Rooij M, Karch JD, Fokkema M, Bakk Z, Pratiwi BC, Kelderman H (2023). SEM-Based --Sample Predictions, Structural Equation Modeling: Multidisciplinary Journal, 30:1, 132-148. <https://doi.org/10.1080/10705511.2022.2061494> Grassi M, Palluzzi F, Tarantino B (2022). SEMgraph: R Package Causal Network Analysis High-Throughput Data Structural Equation Models. Bioinformatics, 38 (20), 4829–4830. <https://doi.org/10.1093/bioinformatics/btac567> Grassi, M., Tarantino, B. (2025). SEMdag: Fast learning Directed Acyclic Graphs via node layer ordering. PLoS ONE 20(1): e0317283. https://doi.org/10.1371/journal.pone.0317283","code":""},{"path":"/reference/predict.SEM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/predict.SEM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"","code":"# load ALS data data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- alsData$group  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  # predictors, source+mediator; outcomes, mediator+sink  ig <- alsData$graph; gplot(ig)  sem0 <- SEMrun(ig, data[train,], algo=\"ricf\", n_rep=0) #> RICF solver ended normally after 2 iterations  #>  #> deviance/df: 6.262846  srmr: 0.3040025  #>  pred0 <- predict(sem0, newdata=data[-train,], verbose=TRUE)  #>      amse        r2      srmr  #> 0.8571886 0.1428114 0.2948502   # predictors, source+mediator+group; outcomes, source+mediator+sink  ig1 <- mapGraph(ig, type = \"group\"); gplot(ig1)  data1 <- cbind(group, data); head(data1[,5]) #>      ALS2      ALS3      ALS4      ALS5      ALS6      ALS7  #> 0.4530701 0.6762093 0.5613048 0.5064807 0.1360061 0.7577341  sem1 <- SEMrun(ig1, data1[train,], algo=\"ricf\", n_rep=0) #> RICF solver ended normally after 2 iterations  #>  #> deviance/df: 6.210737  srmr: 0.2857586  #>  pred1 <- predict(sem1, newdata= data1[-train,], verbose=TRUE)  #>      amse        r2      srmr  #> 0.8581333 0.1418667 0.2849738   # predictors, source nodes; outcomes, sink nodes  ig2 <- mapGraph(ig, type = \"source\"); gplot(ig2)  sem2 <- SEMrun(ig2, data[train,], algo=\"ricf\", n_rep=0) #> RICF solver ended normally after 2 iterations  #>  #> deviance/df: 10.16805  srmr: 0.1282444  #>  pred2 <- predict(sem2, newdata=data[-train,], verbose=TRUE) #>      amse        r2      srmr  #> 0.7241131 0.2758869 0.2062241"},{"path":"/reference/SEMdnn.html","id":null,"dir":"Reference","previous_headings":"","what":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"function builds four Deep Neural Networks (DNN) models based topological structure input graph using 'torch' language. torch package native R, computationally efficient installation simple, need install Python API, DNNs can trained CPU, GPU MacOS GPUs. order install torch please follow steps: install.packages(\"torch\") library(torch) install_torch(reinstall = TRUE) setup GPU problems installing torch package, check installation help torch developer.","code":""},{"path":"/reference/SEMdnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"","code":"SEMdnn(   graph,   data,   outcome = NULL,   algo = \"layerwise\",   hidden = c(10L, 10L, 10L),   link = \"selu\",   bias = TRUE,   dropout = 0,   loss = \"mse\",   validation = 0,   lambda = 0,   alpha = 0.5,   optimizer = \"adam\",   lr = 0.01,   batchsize = NULL,   burnin = 30,   thr = NULL,   nboot = 0,   epochs = 100,   patience = 100,   device = \"cpu\",   verbose = FALSE,   ... )"},{"path":"/reference/SEMdnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"graph igraph object. data matrix rows corresponding subjects, columns graph nodes (variables). outcome character vector (.factor) labels categorical output (target). NULL (default), categorical output (target) considered. algo character value, indicating DNN algorithm: \"nodewise\", \"layerwise\" (default), \"structured\", \"neuralgraph\" (see details). hidden hidden units layers; number layers corresponds length hidden units. default, hidden = c(10L, 10L, 10L). link character value describing activation function use, might single length vector many activation functions assigned layer. default, link = \"selu\". bias logical vector, indicating whether employ biases layers, can either vectors logicals layer (number hidden layers + 1 (final layer)) length one.  default, bias = TRUE. dropout numerical value dropout rate, probability node excluded training.  default, dropout = 0. loss character value specifying network optimized. regression problem used SEMdnn(), user can specify: () \"mse\" (mean squared error), \"mae\" (mean absolute error), \"nnl\" (negative log-likelihood). default, loss = \"mse\". validation numerical value indicating proportion data set used validation set (randomly selected, default = 0). lambda numerical value, indicating strength regularization, \\(\\lambda\\)(L1 + L2) lambda penalty (default = 0). alpha numerical value, add L1/L2 regularization training. Set alpha parameter layer (1-\\(\\alpha\\))L1 + \\(\\alpha\\)L2. must fall 0 1 (default = 0.5). optimizer character value, indicating optimizer use training network. user can specify: \"adam\" (ADAM algorithm), \"adagrad\" (adaptive gradient algorithm), \"rmsprop\" (root mean squared propagation), \"rprop” (resilient backpropagation), \"sgd\" (stochastic gradient descent). default, optimizer = \"adam\". lr numerical value, indicating learning rate given optimizer (default = 0.01). batchsize Number samples used calculate one learning rate step (default = 1/10 training data). burnin Training aborted trainings loss baseline loss burnin epochs (default = 30). thr numeric value [0-1] indicating threshold apply Olden's connection weights color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(connection weights)). nboot number bootstrap samples used compute cheap (lower, upper) CIs input variable weights. default, nboot = 0. epochs numerical value indicating epochs training conducted (default = 100). patience numeric value, training terminate loss increases predetermined number consecutive epochs apply validation loss available. Default patience = 100, early stopping applied. device character value describing CPU/GPU device (\"cpu\", \"cuda\", \"mps\")  neural network trained . default, device = \"cpu\". verbose training loss values DNN model displayed output, comparing training, validation baseline last epoch (default = FALSE). ... Currently ignored.","code":""},{"path":"/reference/SEMdnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"S3 object class \"DNN\" returned. list 5 objects: \"fit\", list DNN model objects, including: estimated covariance matrix (Sigma), estimated model errors (Psi), fitting indices (fitIdx), parameterEstimates, .e., data.frame Olden's connection weights. \"gest\", data.frame estimated connection weights (parameterEstimates) outcome levels, outcome != NULL. \"model\", list MLP network models fitted torch. \"graph\", induced DAG input graph mapped data variables. DAG colored based Olden's connection weights (W), abs(W) > thr W < 0, edge inhibited highlighted blue; otherwise, abs(W) > thr W > 0, edge activated highlighted red. outcome vector given, nodes absolute connection weights summed outcome levels, .e. sum(abs(W[outcome levels])) > thr, highlighted pink. \"data\", input data subset mapping graph nodes.","code":""},{"path":"/reference/SEMdnn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"Four Deep Neural Networks (DNNs) trained SEMdnn(). algo = \"nodewise\", set DNN models performed equation--equation (r=1,...,R) times, R number response (outcome) variables (.e., nodes input graph non-zero incoming connectivity) predictor (input) variables nodes direct edge outcome nodes, poposed various authors causal discovery methods (see Zheng et al, 2020). Note, model learning can time-consuming large graphs large R outcomes. algo = \"layerwise\" (default), set DNN models defined based topological layer structure (j=1,…,L) sink source nodes input graph. iteration, response (output) variables, y nodes j=1,...,(L-1) layer, predictor (input) variables, x nodes belonging successive: (j+1),...,L layers, linked direct edge response variables (see Grassi & Tarantino, 2025). algo = \"structured\", Structured Neural Network (StrNN) defined input output units equal D, number nodes. algorithm uses prior knowledge input graph build neural network architecture via per-layer masking neural weights (.e., W1 * M1, W2 * M2, ..., WL *ML), constraint (W1 * M1) x (W2 * M2) x ... x (WL * ML) = , adjacency matrix input graph (see Chen et al, 2023). algo = \"neuralgraph\", Neural Graphical Model (NGM) generated. StrNN input output units equal D, number nodes. prior knowledge input graph used compute product absolute value neural weights (.e., W = |W1| x |W2| x ... x |WL|), constraint log(W * Ac) = 0, Ac represents complement adjacency matrix input graph, essentially replaces 0 1 vice-versa (see Shrivastava & Chajewska, 2023). DNN model (R \"nodewise\", L<R \"layerwise\", 1 \"structured\" \"neuralgraph\") Multilayer Perceptron (MLP) network, every neuron node connected every neuron node hidden layer every hidden layer . neuron's value determined calculating weighted summation outputs hidden layer , applying activation function.  calculated value every neuron used input neurons layer , output layer reached. boot != 0, function implement cheap bootstrapping proposed Lam (2002) generate uncertainties (.e., bootstrap 90%CIs) DNN parameters. Bootstrapping can enabled setting small number (1 10) bootstrap samples. Note, however, computation can time-consuming massive DNNs, even cheap bootstrapping!","code":""},{"path":"/reference/SEMdnn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"Zheng, X., Dan, C., Aragam, B., Ravikumar, P., Xing E. (2020). Learning sparse nonparametric dags. International conference artificial intelligence statistics, PMLR, 3414-3425. https://doi.org/10.48550/arXiv.1909.13189 Grassi, M., Tarantino, B. (2025). SEMdag: Fast learning Directed Acyclic Graphs via node layer ordering. PLoS ONE 20(1): e0317283. https://doi.org/10.1371/journal.pone.0317283 Chen ., Shi, R.., Gao, X., Baptista, R., Krishnan, R.G. (2023). Structured neural networks density estimation causal inference. Advances Neural Information Processing Systems, 36, 66438-66450. https://doi.org/10.48550/arXiv.2311.02221 Shrivastava, H., Chajewska, U. (2023). Neural graphical models. European Conference Symbolic Quantitative Approaches Uncertainty (pp. 284-307). Cham: Springer Nature Switzerland. https://doi.org/10.48550/arXiv.2210.00453 Lam, H. (2022). Cheap Bootstrap Input Uncertainty Quantification. Winter Simulation Conference (WSC), Singapore, 2022, pp. 2318-2329. https://doi.org/10.1109/WSC57314.2022.10015362","code":""},{"path":"/reference/SEMdnn.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/SEMdnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SEM train with Deep Neural Netwok (DNN) models — SEMdnn","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data group<- alsData$group  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data)) #ncores<- parallel::detectCores(logical = FALSE)  start<- Sys.time() dnn0<- SEMdnn(ig, data[train, ], algo = \"layerwise\",       hidden = c(10,10,10), link = \"selu\", bias = TRUE,       epochs = 32, patience = 10, verbose = TRUE) end<- Sys.time() print(end-start)  #str(dnn0, max.level=2) dnn0$fit$fitIdx parameterEstimates(dnn0$fit) gplot(dnn0$graph) table(E(dnn0$graph)$color)  #...with source nodes -> graph layer structure -> sink nodes  #Topological layer (TL) ordering K<- c(12,  5,  3,  2,  1,  8) K<- rev(K[-c(1,length(K))]);K  ig1<- mapGraph(ig, type=\"source\"); gplot(ig1)  start<- Sys.time() dnn1<- SEMdnn(ig1, data[train, ], algo = \"layerwise\",       hidden = 5*K, link = \"selu\", bias = TRUE,     epochs = 32, patience = 10, verbose = TRUE) end<- Sys.time() print(end-start)  #Visualization of the neural network structure nplot(dnn1, hidden = 5*K, bias = FALSE)  #str(dnn1, max.level=2) dnn1$fit$fitIdx mean(dnn1$fit$Psi) parameterEstimates(dnn1$fit) gplot(dnn1$graph) table(E(dnn1$graph)$color)  #...with a categorical outcome outcome<- factor(ifelse(group == 0, \"control\", \"case\")); table(outcome)   start<- Sys.time() dnn2<- SEMdnn(ig, data[train, ], outcome[train], algo = \"layerwise\",       hidden = c(10,10,10), link = \"selu\", bias = TRUE,       epochs = 32, patience = 10, verbose = TRUE) end<- Sys.time() print(end-start)  #str(dnn2, max.level=2) dnn2$fit$fitIdx parameterEstimates(dnn2$fit) gplot(dnn2$graph)  table(E(dnn2$graph)$color) table(V(dnn2$graph)$color) } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> Running SEM model via DNN... #>  #> layer 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 ... #>     train       val      base  #> 0.3801113       Inf 0.9875000  #>  #> layer 2 : z842 z1432 z5600 z5603 z6300  #>    train      val     base  #> 0.503531      Inf 0.987500  #>  #> layer 3 : z54205 z5606 z5608  #>     train       val      base  #> 0.5394681       Inf 0.9875000  #>  #> layer 4 : z596 z4217  #>     train       val      base  #> 0.8956626       Inf 0.9875001  #>  #> layer 5 : z1616  #>     train       val      base  #> 0.8532046       Inf 0.9875001  #>  done. #>  #> DNN solver ended normally after 160 iterations #>  #>  logL:-40.884373  srmr:0.187098 #> Time difference of 17.30914 secs   #> Running SEM model via DNN... #>  #> layer 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 ... #>     train       val      base  #> 0.4475542       Inf 0.9875000  #>  done. #>  #> DNN solver ended normally after 32 iterations #>  #>  logL:-20.963655  srmr:0.158763 #> Time difference of 16.0658 secs   #> Running SEM model via DNN... #>  #> layer 1 : zcase zcontrol  #>       train         val        base  #> 0.003194495         Inf 0.987500012  #>  #> layer 2 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 ... #>     train       val      base  #> 0.3788583       Inf 0.9875000  #>  #> layer 3 : z842 z1432 z5600 z5603 z6300  #>     train       val      base  #> 0.4717255       Inf 0.9875000  #>  #> layer 4 : z54205 z5606 z5608  #>     train       val      base  #> 0.4962046       Inf 0.9875000  #>  #> layer 5 : z596 z4217  #>     train       val      base  #> 0.9186465       Inf 0.9875001  #>  #> layer 6 : z1616  #>     train       val      base  #> 0.8630623       Inf 0.9875001  #>  done. #>  #> DNN solver ended normally after 192 iterations #>  #>  logL:-39.110385  srmr:0.188061 #> Time difference of 26.72527 secs  #>  #>  pink white  #>     5    26  # }"},{"path":"/reference/SEMml.html","id":null,"dir":"Reference","previous_headings":"","what":"Nodewise SEM train using Machine Learning (ML) — SEMml","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"function converts graph collection nodewise-based models: mediator sink variable can expressed function parents. Based assumed type relationship, .e. linear non-linear, SEMml() fits ML model node (variable) non-zero incoming connectivity. model fitting performed equation-equation (r=1,...,R) times, R number mediators sink nodes.","code":""},{"path":"/reference/SEMml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"","code":"SEMml(   graph,   data,   outcome = NULL,   algo = \"sem\",   thr = NULL,   nboot = 0,   ncores = 2,   verbose = FALSE,   ... )"},{"path":"/reference/SEMml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"graph igraph object. data matrix rows corresponding subjects, columns graph nodes (variables). outcome character vector (.fctor) labels categorical output (target). NULL (default), categorical output (target) considered. algo ML method used nodewise-based predictions. Four algorithms can specified: algo=\"sem\" (default) linear SEM, see SEMrun. algo=\"tree\" CART model, see rpart. algo=\"rf\" random forest model, see ranger. algo=\"xgb\" XGBoost model, see xgboost. thr numeric value [0-1] indicating threshold apply variable importance values color graph. thr = NULL (default), threshold set thr = 0.5*max(abs(variable importance values)). nboot number bootstrap samples used compute cheap (lower, upper) CIs input variable weights. default, nboot = 0. ncores number cpu cores (default = 2) verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/SEMml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"S3 object class \"ML\" returned. list 5 objects: \"fit\", list ML model objects, including: estimated covariance matrix (Sigma), estimated model errors (Psi), fitting indices (fitIdx), parameterEstimates, .e., variable importance measures (VarImp). \"gest\", data.frame variable importances (parameterEstimates) outcome levels, outcome != NULL. \"model\", list fitted non-linear nodewise-based models (tree, rf, xgb, nn dnn). \"graph\", induced DAG input graph  mapped data variables. DAG colored edge/nodes based variable importance measures, .e., abs(VarImp) > thr highlighted red (VarImp > 0) blue (VarImp < 0). outcome vector given, nodes variable importances summed outcome levels, .e. sum(VarImp[outcome levels])) > thr, highlighted pink. \"data\", input data subset mapping graph nodes. Using default algo=\"sem\", usual output linear nodewise-based SEM, see SEMrun (algo=\"cggm\"), returned.","code":""},{"path":"/reference/SEMml.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"mapping data onto input graph, SEMml() creates set nodewise models based directed links, .e., set edges pointing direction, two nodes input graph causally relevant . mediator sink variables defined functions parents. , ML model (sem, tree, rf, xgb) can fitted variable non-zero inbound connectivity. model fitting process performed equation--equation (r=1,...,R) times, R represents number mediators sink nodes input graph. boot != 0, function implement cheap bootstrapping proposed Lam (2002) generate uncertainties (.e., bootstrap 90%CIs) ML parameters. Bootstrapping can enabled setting small number (1 10) bootstrap samples. Note, however, computation can time-consuming massive MLs, even cheap bootstrapping!","code":""},{"path":"/reference/SEMml.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"Grassi M., Palluzzi F., Tarantino B. (2022). SEMgraph: R Package Causal Network Analysis High-Throughput Data Structural Equation Models. Bioinformatics, 38 (20), 4829–4830 <https://doi.org/10.1093/bioinformatics/btac567> Breiman L., Friedman J.H., Olshen R.., Stone, C.J. (1984) Classification Regression Trees. Chapman Hall/CRC. Breiman L. (2001). Random Forests, Machine Learning 45(1), 5-32. Chen T., Guestrin C. (2016). XGBoost: Scalable Tree Boosting System. Proceedings 22nd ACM SIGKDD International Conference Knowledge Discovery Data Mining. Lam, H. (2022). Cheap bootstrap input uncertainty quantification. WSC '22: Proceedings Winter Simulation Conference, 2318-2329.","code":""},{"path":"/reference/SEMml.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/SEMml.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nodewise SEM train using Machine Learning (ML) — SEMml","text":"","code":"# \\donttest{ # Load Amyotrophic Lateral Sclerosis (ALS) ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- alsData$group  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  start<- Sys.time() # ... tree res1<- SEMml(ig, data[train, ], algo=\"tree\") #> Running SEM model via ML... #>  done. #>  #> TREE solver ended normally after 23 iterations #>  #>  logL:-45.080145  srmr:0.201877  # ... rf res2<- SEMml(ig, data[train, ], algo=\"rf\") #> Running SEM model via ML... #>  done. #>  #> RF solver ended normally after 23 iterations #>  #>  logL:-33.16687  srmr:0.086188  # ... xgb res3<- SEMml(ig, data[train, ], algo=\"xgb\") #> Running SEM model via ML... #>  done. #>  #> XGB solver ended normally after 23 iterations #>  #>  logL:70.10035  srmr:0.001439  # ... sem res4<- SEMml(ig, data[train, ], algo=\"sem\") #> Running SEM model via ML... #>  done. #>  #> SEM solver ended normally after 23 iterations #>  #>  logL:-48.441286  srmr:0.306438  end<- Sys.time() print(end-start) #> Time difference of 6.972608 secs  #visualizaation of the colored dag for algo=\"sem\" gplot(res4$graph, l=\"dot\", main=\"sem\")   #Comparison of fitting indices (in train data) res1$fit$fitIdx #tree #>        logL        amse        rmse        srmr  #> -45.0801450   0.6403463   0.8002164   0.2018772  res2$fit$fitIdx #rf #>         logL         amse         rmse         srmr  #> -33.16686964   0.23129978   0.48093636   0.08618768  res3$fit$fitIdx #xgb #>         logL         amse         rmse         srmr  #> 7.010035e+01 1.865892e-04 1.365977e-02 1.439282e-03  res4$fit$fitIdx #sem #>        logL        amse        rmse        srmr  #> -48.4412858   0.8574470   0.9259843   0.3064378   #Comparison of parameter estimates (in train data) parameterEstimates(res1$fit) #tree #>      lhs op   rhs VarImp lower upper #> 1  10452  ~  6647 16.112     0     0 #> 43 84134  ~  6647 24.516     0     0 #> 38   596  ~  6647 10.728     0     0 #> 17  4747  ~  6647 19.529     0     0 #> 41 79139  ~  6647 25.164     0     0 #> 27  5530  ~  6647 27.370     0     0 #> 28  5532  ~  6647 32.388     0     0 #> 29  5533  ~  6647 20.453     0     0 #> 30  5534  ~  6647 30.462     0     0 #> 31  5535  ~  6647 25.923     0     0 #> 44   842  ~ 54205 32.106     0     0 #> 23 54205  ~   581 35.206     0     0 #> 24 54205  ~   572  6.647     0     0 #> 25 54205  ~   596  6.135     0     0 #> 26 54205  ~   598 15.418     0     0 #> 45   842  ~   317  6.470     0     0 #> 42   836  ~   842 28.768     0     0 #> 4   1616  ~  7132 14.271     0     0 #> 5   1616  ~  7133 19.620     0     0 #> 6   4217  ~  1616 12.718     0     0 #> 36  5606  ~  4217 11.445     0     0 #> 37  5608  ~  4217 35.143     0     0 #> 2   1432  ~  5606 15.674     0     0 #> 32  5600  ~  5606 12.193     0     0 #> 34  5603  ~  5606 14.378     0     0 #> 39  6300  ~  5606 11.595     0     0 #> 3   1432  ~  5608 22.355     0     0 #> 33  5600  ~  5608 19.935     0     0 #> 35  5603  ~  5608 13.768     0     0 #> 40  6300  ~  5608 25.313     0     0 #> 18  4747  ~  1432 22.556     0     0 #> 7   4741  ~  1432 24.848     0     0 #> 12  4744  ~  1432 19.416     0     0 #> 19  4747  ~  5600 11.291     0     0 #> 8   4741  ~  5600  7.237     0     0 #> 13  4744  ~  5600 16.367     0     0 #> 20  4747  ~  5603 13.228     0     0 #> 9   4741  ~  5603 13.976     0     0 #> 14  4744  ~  5603 17.849     0     0 #> 21  4747  ~  6300  3.397     0     0 #> 10  4741  ~  6300  2.549     0     0 #> 15  4744  ~  6300 10.920     0     0 #> 22  4747  ~  5630  8.191     0     0 #> 11  4741  ~  5630  9.045     0     0 #> 16  4744  ~  5630 10.093     0     0 parameterEstimates(res2$fit) #rf #>      lhs op   rhs VarImp lower upper #> 1  10452  ~  6647  0.097     0     0 #> 43 84134  ~  6647  0.445     0     0 #> 38   596  ~  6647  0.117     0     0 #> 17  4747  ~  6647  0.254     0     0 #> 41 79139  ~  6647  0.665     0     0 #> 27  5530  ~  6647  0.338     0     0 #> 28  5532  ~  6647  0.480     0     0 #> 29  5533  ~  6647  0.317     0     0 #> 30  5534  ~  6647  0.502     0     0 #> 31  5535  ~  6647  0.383     0     0 #> 44   842  ~ 54205  0.420     0     0 #> 23 54205  ~   581  0.569     0     0 #> 24 54205  ~   572  0.053     0     0 #> 25 54205  ~   596 -0.009     0     0 #> 26 54205  ~   598  0.214     0     0 #> 45   842  ~   317  0.133     0     0 #> 42   836  ~   842  0.479     0     0 #> 4   1616  ~  7132  0.060     0     0 #> 5   1616  ~  7133  0.055     0     0 #> 6   4217  ~  1616  0.026     0     0 #> 36  5606  ~  4217 -0.143     0     0 #> 37  5608  ~  4217  0.697     0     0 #> 2   1432  ~  5606  0.132     0     0 #> 32  5600  ~  5606 -0.037     0     0 #> 34  5603  ~  5606  0.181     0     0 #> 39  6300  ~  5606  0.014     0     0 #> 3   1432  ~  5608  0.456     0     0 #> 33  5600  ~  5608  0.110     0     0 #> 35  5603  ~  5608  0.043     0     0 #> 40  6300  ~  5608  0.225     0     0 #> 18  4747  ~  1432  0.086     0     0 #> 7   4741  ~  1432  0.250     0     0 #> 12  4744  ~  1432  0.180     0     0 #> 19  4747  ~  5600  0.098     0     0 #> 8   4741  ~  5600  0.093     0     0 #> 13  4744  ~  5600  0.167     0     0 #> 20  4747  ~  5603  0.105     0     0 #> 9   4741  ~  5603  0.109     0     0 #> 14  4744  ~  5603  0.085     0     0 #> 21  4747  ~  6300  0.025     0     0 #> 10  4741  ~  6300 -0.025     0     0 #> 15  4744  ~  6300  0.025     0     0 #> 22  4747  ~  5630  0.033     0     0 #> 11  4741  ~  5630  0.050     0     0 #> 16  4744  ~  5630  0.054     0     0 parameterEstimates(res3$fit) #xgb #>      lhs op   rhs VarImp lower upper #> 1  10452  ~  6647  1.000     0     0 #> 43 84134  ~  6647  1.000     0     0 #> 38   596  ~  6647  1.000     0     0 #> 17  4747  ~  6647  0.263     0     0 #> 41 79139  ~  6647  1.000     0     0 #> 27  5530  ~  6647  1.000     0     0 #> 28  5532  ~  6647  1.000     0     0 #> 29  5533  ~  6647  1.000     0     0 #> 30  5534  ~  6647  1.000     0     0 #> 31  5535  ~  6647  1.000     0     0 #> 44   842  ~ 54205  0.694     0     0 #> 23 54205  ~   581  0.632     0     0 #> 24 54205  ~   572  0.104     0     0 #> 25 54205  ~   596  0.048     0     0 #> 26 54205  ~   598  0.216     0     0 #> 45   842  ~   317  0.306     0     0 #> 42   836  ~   842  1.000     0     0 #> 4   1616  ~  7132  0.430     0     0 #> 5   1616  ~  7133  0.570     0     0 #> 6   4217  ~  1616  1.000     0     0 #> 36  5606  ~  4217  1.000     0     0 #> 37  5608  ~  4217  1.000     0     0 #> 2   1432  ~  5606  0.527     0     0 #> 32  5600  ~  5606  0.409     0     0 #> 34  5603  ~  5606  0.521     0     0 #> 39  6300  ~  5606  0.381     0     0 #> 3   1432  ~  5608  0.473     0     0 #> 33  5600  ~  5608  0.591     0     0 #> 35  5603  ~  5608  0.479     0     0 #> 40  6300  ~  5608  0.619     0     0 #> 18  4747  ~  1432  0.300     0     0 #> 7   4741  ~  1432  0.397     0     0 #> 12  4744  ~  1432  0.266     0     0 #> 19  4747  ~  5600  0.092     0     0 #> 8   4741  ~  5600  0.142     0     0 #> 13  4744  ~  5600  0.349     0     0 #> 20  4747  ~  5603  0.194     0     0 #> 9   4741  ~  5603  0.264     0     0 #> 14  4744  ~  5603  0.202     0     0 #> 21  4747  ~  6300  0.036     0     0 #> 10  4741  ~  6300  0.028     0     0 #> 15  4744  ~  6300  0.047     0     0 #> 22  4747  ~  5630  0.115     0     0 #> 11  4741  ~  5630  0.168     0     0 #> 16  4744  ~  5630  0.136     0     0 parameterEstimates(res4$fit) #sem #>      lhs op   rhs VarImp lower upper #> 1  10452  ~  6647  0.411     0     0 #> 2  84134  ~  6647  2.776     0     0 #> 8    596  ~  6647 -0.653     0     0 #> 24  4747  ~  6647  5.331     0     0 #> 40 79139  ~  6647  4.441     0     0 #> 41  5530  ~  6647  3.094     0     0 #> 42  5532  ~  6647  4.277     0     0 #> 43  5533  ~  6647 -2.575     0     0 #> 44  5534  ~  6647  3.120     0     0 #> 45  5535  ~  6647 -4.107     0     0 #> 9    842  ~ 54205 -6.545     0     0 #> 3  54205  ~   581 -4.883     0     0 #> 4  54205  ~   572  0.161     0     0 #> 5  54205  ~   596 -0.284     0     0 #> 6  54205  ~   598 -1.702     0     0 #> 10   842  ~   317 -2.667     0     0 #> 7    836  ~   842 -4.841     0     0 #> 11  1616  ~  7132  1.662     0     0 #> 12  1616  ~  7133  0.367     0     0 #> 13  4217  ~  1616 -1.689     0     0 #> 14  5606  ~  4217 -0.828     0     0 #> 15  5608  ~  4217  5.921     0     0 #> 16  1432  ~  5606  3.182     0     0 #> 18  5600  ~  5606  1.493     0     0 #> 20  5603  ~  5606  2.255     0     0 #> 22  6300  ~  5606  1.168     0     0 #> 17  1432  ~  5608  5.564     0     0 #> 19  5600  ~  5608 -2.371     0     0 #> 21  5603  ~  5608  0.181     0     0 #> 23  6300  ~  5608 -3.871     0     0 #> 25  4747  ~  1432  2.995     0     0 #> 30  4741  ~  1432  2.889     0     0 #> 35  4744  ~  1432  2.522     0     0 #> 26  4747  ~  5600  0.252     0     0 #> 31  4741  ~  5600 -0.730     0     0 #> 36  4744  ~  5600 -0.738     0     0 #> 27  4747  ~  5603  2.135     0     0 #> 32  4741  ~  5603  0.870     0     0 #> 37  4744  ~  5603  0.588     0     0 #> 28  4747  ~  6300  1.023     0     0 #> 33  4741  ~  6300  0.891     0     0 #> 38  4744  ~  6300  0.572     0     0 #> 29  4747  ~  5630 -1.275     0     0 #> 34  4741  ~  5630 -1.892     0     0 #> 39  4744  ~  5630 -2.107     0     0  #Comparison of VarImp (in train data) table(E(res1$graph)$color) #tree #>  #> gray50   red2  #>     25     20  table(E(res2$graph)$color) #rf #>  #> gray50   red2  #>     35     10  table(E(res3$graph)$color) #xgb #>  #> gray50   red2  #>     25     20  table(E(res4$graph)$color) #sem #>  #>     gray50       red2 royalblue3  #>         35          5          5   #Comparison of AMSE, R2, SRMR (in test data) print(predict(res1, data[-train, ])$PE) #tree #>       amse         r2       srmr  #> 0.93770362 0.06229638 0.22693793  print(predict(res2, data[-train, ])$PE) #rf #>       amse         r2       srmr  #> 0.92435872 0.07564128 0.17600941  print(predict(res3, data[-train, ])$PE) #xgb #>       amse         r2       srmr  #> 0.97398929 0.02601071 0.15433519  print(predict(res4, data[-train, ])$PE) #sem #>      amse        r2      srmr  #> 0.8572186 0.1427814 0.2972509   #...with a categorical (as.factor) outcome outcome <- factor(ifelse(group == 0, \"control\", \"case\")); table(outcome)  #> outcome #>    case control  #>     139      21   res5 <- SEMml(ig, data[train, ], outcome[train], algo=\"tree\") #> Running SEM model via ML... #>  done. #>  #> TREE solver ended normally after 25 iterations #>  #>  logL:-48.72171  srmr:0.196861 gplot(res5$graph)  table(E(res5$graph)$color) #>  #> gray50   red2  #>     25     20  table(V(res5$graph)$color) #>  #>  pink white  #>     2    11   pred <- predict(res5, data[-train, ], outcome[-train], verbose=TRUE) #>       amse         r2       srmr  #> 0.91133182 0.08866818 0.22538176  yhat <- pred$Yhat[ ,levels(outcome)]; head(yhat) #>         case    control #> 1  0.4773726 -0.4773726 #> 2  0.4773726 -0.4773726 #> 3  0.4773726 -0.4773726 #> 4  0.4773726 -0.4773726 #> 5  0.4773726 -0.4773726 #> 6 -0.6137648  0.6137648 yobs <- outcome[-train]; head(yobs) #> [1] case case case case case case #> Levels: case control classificationReport(yobs, yhat, verbose=TRUE)$stats #>          pred #> yobs      case control #>   case      57      17 #>   control    2       4 #>   #>              precision    recall        f1 accuracy      mcc support #> case         0.9661017 0.7702703 0.8571429   0.7625 0.261562      74 #> control      0.1904762 0.6666667 0.2962963   0.7625 0.261562       6 #> macro avg    0.5782889 0.7184685 0.5767196   0.7625 0.261562      80 #> weighted avg 0.9079298 0.7625000 0.8150794   0.7625 0.261562      80 #>              support_prop #> case                0.925 #> control             0.075 #> macro avg           1.000 #> weighted avg        1.000 # }"},{"path":"/news/index.html","id":"version-110-release-notes","dir":"Changelog","previous_headings":"","what":"Version 1.1.0 Release Notes","title":"Version 1.1.0 Release Notes","text":"new revised torch functions longer require “cito” package. Added new argument algo = c(“nodewise”,“layerwise”,“structured”,“neuralgraph”) SEMdnn() function. Four algorithms now implemented using R MLPs (number nodes non-zero incoming connectivity) “nodewise”, L<R MLPs (number layers input graph) “layerwise”, 1 MLP “structured” “neuralgraph”. Delete algo=“nn” algo=“dnn” SEMml() function. NNs small neural network model (1 hidden layer 10 nodes) large neural network model (1 hidden layers 1000 nodes) run algo=“nodewise” SEMdnn() function. Added new function getLOCO(). SEMml() algo=c(“sem”,“tree”,“rf”,“xgb”), computes contributions variable individual predictions using LOCO (Leave COvariates) values based ghost variables. CPU-efficient procedure. Various fixed bugs discovered release 1.0.0.","code":""},{"path":"/news/index.html","id":"version-100-release-notes","dir":"Changelog","previous_headings":"","what":"Version 1.0.0 Release Notes","title":"Version 1.0.0 Release Notes","text":"CRAN release: 2025-01-23 Version 1.0.0 major release several new features, including: Added new argument outcome = NULL (defult). parameter used SEMdnn() SEMml() functions process sink categorical node (factor) classification purposes using graph nodes covariates. Added new argument newoutcome = NULL (defult). parameter used predict (.SEM, .DNN, .ML) functions predict sink categorical node (factor) classification purposes using graph nodes covariates. classificationReport() function. report showing main classification metrics, like precision, recall, F1-score, accuracy, Matthew’s correlation coefficient (mcc) classes node = .factor(outcome). crossValidation() function. R-repeated K-fold cross-validation list M models SEMrun(), SEMml() SEMdnn(). winning model selected reporting mean predicted performances across RxKxM runs. getVariableImportance() function. Extraction common Machine Learning (ML) variable (predictor) importance measures fitting SEMrun(), SEMml() SEMdnn() models. Added new argument nboot = 0 (default). parameter implements cheap bootstrapping SEMdnn() SEMml() functions generate uncertainties, .e. CIs, DNN/ML parameters. Bootstrapping can enabled setting small number (1 10) bootstrap samples. Change argument thr = 0.5 * max(abs(parameters)) (default). Now DAG can colored using numeric [0-1] threshold. example, 1/0.5 = 2, can interpreted number times node/edge parameter less maximum parameter value. Various fixed bugs discovered release 0.1.0.","code":""},{"path":"/news/index.html","id":"version-010-release-notes","dir":"Changelog","previous_headings":"","what":"Version 0.1.0 Release Notes","title":"Version 0.1.0 Release Notes","text":"CRAN release: 2024-09-16 First stable version CRAN","code":""}]

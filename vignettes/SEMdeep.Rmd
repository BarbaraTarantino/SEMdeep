---
title: "SEMdeep"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SEMdeep}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(SEMdeep)
```

# SEMdeep tutorial

The following section offers an overview of **SEMdeep** functionalities. Starting from ML (or DNN) model training, it will gently introduce functions for model evaluation and model explainability. This section includes:

1.  **The ALS dataset**
2.  **Model training and validation**
3.  **Binary outcome variable**
4.  **Variable importance functions**

Please, also visit our website [**HERE**](https://barbaratarantino.github.io/SEMdeep/) for a complete list of SEMdeep functions and related examples.

## 1. The ALS dataset

**SEMdata** provides the ALS RNA-seq dataset of 139 cases and 21 healthy controls, from Tam O.H. *et al.*, 2019 (GEO accession: [GSE124439](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE124439)). Raw data were pre-processed applying batch effect correction, using the sva R package (Leek et al., 2012), to remove data production center and brain area biases. Using multidimensional scaling-based clustering, ALS-specific and HC-specific clusters were generated. Misclassified samples were blacklisted and removed from the dataset. Since the expression of many genes is significantly different from Gaussian, we apply a nonparanormal transform to relax the normality assumption, with the **SEMgraph** package.

```{r}

# ALS input graph
ig<- alsData$graph; gplot(ig)
summary(ig)

# ALS RNA-seq expression data
data<- alsData$exprs; dim(data)
data<- transformData(data)$data

# group = {1: case, 0: control} vector
group<- alsData$group; table(group)
```

## 2. Model training and validation

**SEMdeep** offers two main modes of model training: (i) Nodewise-predictive SEM train using Machine Learning (ML), (ii) Layer-wise SEM train with a Deep Neural Netwok (DNN). In detail:

(i) **Nodewise-predictive SEM train using Machine Learning (SEMml())**. The mediator or sink variables can be characterized as functions of their parents. An ML model (sem, gam, rf, xgb, nn, dnn) can then be fitted to each variable with non-zero inbound connectivity, taking into account the kind of relationship (linear or non-linear).
(ii) **Layer-wise SEM train with a Deep Neural Netwok (SEMdnn())** the nodes in one layer act as response variables (output) y and the nodes in the sucessive layers act as predictors (input) x.

**SEMdeep** will automatically take care of creating a series of ML (DNN) models to (i) each node (variable) or (ii) each layer (set of variables) in the input graph, applying heuristics and parallelization settings for large graphs. In addition, variable importance computation enables the extraction of important edges (relationships) in the graph.

Within SEMml() workflow, a different technique, specific to the model of interest, is applied: (i) the squared value of the t-statistic or F-statistic of the model parameters for "sem" or "gam"; (ii) the variable importance from the importance or **xgb.importance()** functions for "rf" or "xgb"; (iii) the Olden's connection weights for "nn" or "dnn". Regarding the SEMdnn() function, connection weights of the input node (variables) are computed

```{r}

## ALS model training with SEMml() 

# Divide the data in train-test (0.5-0.5) samples
set.seed(123)
train<- sample(1:nrow(data), 0.5*nrow(data))

# Random forest (algo="rf")

res1 <- SEMml(ig, data, train, algo="rf", vimp=TRUE)
est1 <- res1$fit$parameterEstimates; head(est1)

print(res1$fit$fitIdx) #performance on train data
print(predict(res1, data[-train, ])$PE[1]) #performance on test data

# XGBoost (algo="xgb")
res2 <- SEMml(ig, data, train, algo="xgb", vimp=TRUE)
est2 <- head(res2$fit$parameterEstimates); head(est2)

print(res2$fit$fitIdx) #performance on train data
print(predict(res2, data[-train, ])$PE[1]) #performance on test data

# Neural networks (algo="nn")
res3 <- SEMml(ig, data, train, algo="nn", vimp=TRUE)
est3 <- head(res3$fit$parameterEstimates); head(est3)

print(res3$fit$fitIdx) #performance on train data
print(predict(res3, data[-train, ])$PE[1]) #performance on test data

# Linear regression (algo="sem")
res4<- SEMml(ig, data, train, algo="sem", vimp=TRUE)
est4 <- head(res4$fit$parameterEstimates); head(est4)

print(res4$fit$fitIdx) #performance on train data
print(predict(res4, data[-train, ])$PE[1]) #performance on test data

# Deep Neural Networks (algo="dnn")
res5<- SEMml(ig, data, train, algo="dnn", vimp=TRUE)
est5 <- head(res5$fit$parameterEstimates); head(est5)

print(res5$fit$fitIdx) #performance on train data
print(predict(res5, data[-train, ])$PE[1]) #performance on test data


##--------------------------------------------------------------------##

## ALS model training with SEMdnn()

res6 <- SEMdnn(ig, data, train=train, cowt = TRUE, thr = NULL,
               loss = "mse", hidden = c(10, 10, 10), link = "selu",
               validation = 0, bias = TRUE, lr = 0.01,
               epochs = 32, device = "cpu", verbose = TRUE)
est6 <- head(res6$fit$parameterEstimates$beta); head(est6)

print(res6$fit$fitIdx) #performance on train data
print(predict(res6, data[-train, ])$PE[1]) #performance on test data

## Figure 1. Estimated variable importance graphs (SEMml and SEMdnn) ----##

pdf("Figure1.pdf", width = 9, height = 14)
par(mfrow=c(3,2), mar=rep(1,4))
gplot(res1$graph, l="dot", main="SEMml (rf)", cex.main = 1.5)
gplot(res2$graph, l="dot", main="SEMml (xgb)", cex.main = 1.5)
gplot(res3$graph, l="dot", main="SEMml (nn)", cex.main = 1.5)
gplot(res4$graph, l="dot", main="SEMml (sem)", cex.main = 1.5)
gplot(res5$graph, l="dot", main="SEMml (dnn)", cex.main = 1.5)
gplot(res6$graph, l="dot", main="SEMdnn", cex.main = 1.5)
dev.off()


## Figure 2. Estimated DNN structure for DNN model 1 ----##

pdf("Figure2.pdf", width = 14, height = 9)
nplot(res6$model[[1]], bias=FALSE)
dev.off()
```

**Figure 1. Estimated variable importance graphs (SEMml).** Estimated variable importance plots obtained from SEMml() and SEMdnn() functions where important edges are highlighted in red/pink (positive variable importance score) and blue/lightblue (negative variable importance score).

![](https://github.com/BarbaraTarantino/SEMdeep/blob/master/docs/figures/Figure1.png?raw=true)

**Figure 2. Estimated DNN structure for DNN model 1.** Estimated DNN structure for DNN model 1 obtained from SEMdnn() function with input, hidden and output layers (with optional bias if specified in model structure)

![](https://github.com/BarbaraTarantino/SEMdeep/blob/master/docs/figures/Figure2.png?raw=true)

### 2.1. Cross validation performance

One common technique for assessing how well a machine learning algorithm or configuration performs on a dataset is the k-fold cross-validation process. By reporting the mean result across all folds from all runs and iteratively repeating the cross-validation process, repeated k-fold cross-validation can be also implemented to better understand a machine learning model's predicted performance.

**SEMdeep** offers two main modes of repeated k-fold cross validation: (i) Repeated k-fold cross validation for SEMml() function (SEMml.cv()) and (ii) for SEMdnn() (SEMdnn.cv()). The user can specify different function arguments to create the specific model configuration tu run within the validation process.

```{r}

## ALS model cross validation with SEMml.cv() 

# CV Random forest (algo="rf")
res1.cv<- SEMml.cv(graph=ig, data=data, C=FALSE, K=5, R=2, algo="rf")
print(res1.cv)

# CV XGBoost (algo="xgb")
res2.cv<- SEMml.cv(graph=ig, data=data, C=FALSE, K=5, R=2, algo="xgb")
print(res2.cv)

# CV Neural networks (algo="nn")
res3.cv<- SEMml.cv(graph=ig, data=data, C=FALSE, K=5, R=2, algo="nn")
print(res3.cv)

# CV Linear regression (algo="sem")
res4.cv<- SEMml.cv(graph=ig, data=data, C=FALSE, K=5, R=2, algo="sem")
print(res4.cv)

# CV Deep Neural Networks (algo="dnn")
res5.cv<- SEMml.cv(graph=ig, data=data, C=FALSE, K=5, R=2, algo="dnn")
print(res5.cv)

##--------------------------------------------------------------------##

## ALS model cross validation with SEMdnn.cv() 

res6.cv<- SEMdnn.cv(graph=ig, data=data, C=FALSE, K=5, R=2,
                 hidden = c(10,10,10), link = "selu", epochs = 10,
                 device = "cpu", verbose = FALSE)
print(res6.cv)
```

## 3. Binary outcome variable (1=case, 0=control)

**SEMdeep** offers also the possibility to predict a binary outcome variable representing, for example, case and control subjects. Initially, the user needs to specify the input graph for model training, adding an outcome sink variables connected to the graph structure of interest. This can be done via the mapGraph() function. Then, the user needs to add the binary group vector as the first column of the data matrix and then, repeat the same procedure of section 1 for model training with SEMml() (or SEMml.cv()) and SEMdnn() (or SEMdnn.cv()) functions, but adding as inputs the new graph structure and the new data matrix.

To evaluate the model performance, the benchmark() function allows to calculate a series of binary classification evaluation statistics.

```{r}

# ALS input graph
ig1<- mapGraph(ig, type="outcome"); gplot(ig1)

# group = {1: case, -1: control} outcome vector
outcome<- ifelse(group == 0, -1, 1); table(outcome)

# ALS RNA-seq expression data with outcome variable as first column
data1<- cbind(outcome, data); data1[1:4,1:4]


## ALS model training with SEMml() and binary outcome variable

# Random forest (algo="rf")

res1.1 <- SEMml(ig1, data1, train, algo="rf", vimp=TRUE)
est1.1 <- res1.1$fit$parameterEstimates; tail(est1.1)

mse1.1<- predict(res1.1, data1[-train, ])
yobs<- group[-train]; length(yobs)
yhat<- mse1.1$Yhat[ ,"outcome"]; length(yhat)
benchmark(yobs, yhat, thr=0, F1=TRUE)
benchmark(yobs, yhat, thr=0, F1=FALSE)

# CV Random forest (algo="rf")
rescv1.1<- SEMml.cv(graph=ig1, data=data1, C=TRUE, K=5, R=2, algo="rf")
print(rescv1.1)


# XGBoost (algo="xgb")
res2.1 <- SEMml(ig1, data1, train, algo="xgb", vimp=TRUE)
est2.1 <- res2.1$fit$parameterEstimates; tail(est2.1)

mse2.1<- predict(res2.1, data1[-train, ])
yobs<- group[-train]; length(yobs)
yhat<- mse2.1$Yhat[ ,"outcome"]; length(yhat)
benchmark(yobs, yhat, thr=0, F1=TRUE)
benchmark(yobs, yhat, thr=0, F1=FALSE)

# CV XGBoost (algo="xgb")
rescv2.1<- SEMml.cv(graph=ig1, data=data1, C=TRUE, K=5, R=2, algo="xgb")
print(rescv2.1)


# Neural networks (algo="nn")
res3.1 <- SEMml(ig1, data1, train, algo="nn", vimp=TRUE)
est3.1 <- res3.1$fit$parameterEstimates; tail(est3.1)

mse3.1<- predict(res3.1, data1[-train, ])
yobs<- group[-train]; length(yobs)
yhat<- mse3.1$Yhat[ ,"outcome"]; length(yhat)
benchmark(yobs, yhat, thr=0, F1=TRUE)
benchmark(yobs, yhat, thr=0, F1=FALSE)

# CV Neural networks (algo="nn")
rescv3.1<- SEMml.cv(graph=ig1, data=data1, C=TRUE, K=5, R=2, algo="nn")
print(rescv3.1)


# Linear regression (algo="sem")
res4.1<- SEMml(ig1, data1, train, algo="sem", vimp=TRUE)
est4.1 <- res4.1$fit$parameterEstimates; tail(est4.1)

mse4.1<- predict(res4.1, data1[-train, ])
yobs<- group[-train]; length(yobs)
yhat<- mse4.1$Yhat[ ,"outcome"]; length(yhat)
benchmark(yobs, yhat, thr=0, F1=TRUE)
benchmark(yobs, yhat, thr=0, F1=FALSE)

# CV Linear regression (algo="sem")
rescv4.1<- SEMml.cv(graph=ig1, data=data1, C=TRUE, K=5, R=2, algo="sem")
print(rescv4.1)


# Deep Neural Networks (algo="dnn")
res5.1<- SEMml(ig1, data1, train, algo="dnn", vimp=TRUE)
est5.1 <- res5.1$fit$parameterEstimates; tail(est5.1)

mse5.1<- predict(res5.1, data1[-train, ])
yobs<- group[-train]; length(yobs)
yhat<- mse5.1$Yhat[ ,"outcome"]; length(yhat)
benchmark(yobs, yhat, thr=0, F1=TRUE)
benchmark(yobs, yhat, thr=0, F1=FALSE)

# CV Deep Neural Networks (algo="dnn")
rescv5.1<- SEMml.cv(graph=ig1, data=data1, C=TRUE, K=5, R=2, algo="dnn")
print(rescv5.1)


##--------------------------------------------------------------------##

## ALS model training with SEMdnn() and binary outcome variable

res6.1 <- SEMdnn(ig1, data1, train, cowt = TRUE, thr = NULL,
                 loss = "mse", hidden = c(10, 10, 10), link = "selu",
                 validation = 0, bias = TRUE, lr = 0.01,
                 epochs = 32, device = "cpu", verbose = TRUE)
est6.1 <- res6.1$fit$parameterEstimates$beta; head(est6.1)

mse6.1 <- predict(res6.1, data1[-train, ])
yobs <- group[-train]; length(yobs)
yhat <- mse6.1$Yhat[,"outcome"]; length(yhat)
benchmark(yobs, yhat, thr=0, F1=FALSE)
benchmark(yobs, yhat, thr=0, F1=TRUE)

rescv6.1<- SEMdnn.cv(graph=ig1, data=data1, C=TRUE, K=5, R=2,
                 hidden = c(10,10,10), link = "selu", epochs = 10,
                 device = "cpu", verbose = FALSE)
print(rescv6.1)

```

## 4. Variable importance functions

Interpreting intricate machine learning models is frequently challenging. Nonetheless, comprehending and elucidating the reasoning behind a model's particular forecast is vital in numerous scenarios.

Besides the variable importance metrics already computed in the SEMml() and SEMdnn() functions, **SEMdeep** offers other two main group of methods for the assessment of variable importance:

(i) a model agnostic method: Shapley values computation with the getShapleyR2() function;
(ii) three Neural network specific methods: Connection weight approach with the getConnectionWeigth() function; Gradient weight approach with the getGradientWeight() function and Test for the significance of input variables with the getInputPvalue() function.

```{r}

## Compute agnostic variable importance using Shapley (R2) values on SEMml() output 

sh1<- getShapleyR2(res1, data[-train, ], thr=NULL, verbose=TRUE)
head(sh1$est)

sh2<- getShapleyR2(res2, data[-train, ], thr=NULL, verbose=TRUE)
head(sh2$est)

sh3<- getShapleyR2(res3, data[-train, ], thr=NULL, verbose=TRUE)
head(sh3$est)

sh4<- getShapleyR2(res4, data[-train, ], thr=NULL, verbose=TRUE)
head(sh4$est)

sh5<- getShapleyR2(res5, data[-train, ], thr=NULL, verbose=TRUE)
head(sh5$est)


# average shapley R2 across response variables (rf model)
R2<- abs(sh1$est[,4])
Y<- sh1$est[,1]
R2Y<- aggregate(R2~Y,data=data.frame(R2,Y),FUN="mean")
PE<- predict(res1, data[-train, ])$PE
head(cbind(R2Y=R2Y[,2],PEY=PE[-1]))
mean(R2) # total average R2
PE[1]    # total MSE


##--------------------------------------------------------------------##

# Compute Neural network variable importance on SEMml() (nn or dnn) and SEMdnn() output 

# cw4<- getConnectionWeight(res4, thr=NULL, verbose=TRUE)
# cw5<- getConnectionWeight(res5, thr=NULL, verbose=TRUE)
cw6<- getConnectionWeight(res6, thr=NULL, verbose=TRUE)
head(cw6$est)

# gw4<- getGradientWeight(res4, thr=NULL, verbose=TRUE)
# gw5<- getGradientWeight(res5, thr=NULL, verbose=TRUE)
gw6<- getGradientWeight(res6, thr=NULL, verbose=TRUE)
head(gw6$est)

# get.pv4<- getInputPvalue(res4, thr=NULL, verbose=TRUE)
# get.pv5<- getInputPvalue(res5, thr=NULL, verbose=TRUE)
pv6<- getInputPvalue(res6, thr=NULL, verbose=TRUE)
head(pv6$est)


## Figure 3. Estimated variable importance graphs (Shapley and NN specific) ----##

pdf("Figure3.pdf", width = 9, height = 16)
par(mfrow=c(4,2), mar=rep(1,4))
gplot(sh1$dag, l="dot", main="Shapley (SEMml, rf)", cex.main = 1.5)
gplot(sh2$dag, l="dot", main="Shapley (SEMml, xgb)", cex.main = 1.5)
gplot(sh3$dag, l="dot", main="Shapley (SEMml, nn)", cex.main = 1.5)
gplot(sh4$dag, l="dot", main="Shapley (SEMml, sem)", cex.main = 1.5)
gplot(sh5$dag, l="dot", main="Shapley (SEMml, dnn)", cex.main = 1.5)
gplot(cw6$dag, l="dot", main="Connection weights (SEdnn)", cex.main = 1.5)
gplot(gw6$dag, l="dot", main="Gradient weights (SEMdnn)", cex.main = 1.5)
gplot(pv6$dag, l="dot", main="Significance test (SEMdnn)", cex.main = 1.5)
dev.off()
```

**Figure 3. Estimated variable importance graphs (agnostic and model specific methods).** Estimated variable importance plots obtained from getShapleyR2(), getConnectionWeight(), getGradientWeight() and getInputPvalue() function where important edges are highlighted in red/pink (positive variable importance score) and blue/lightblue (negative variable importance score).

![](https://github.com/BarbaraTarantino/SEMdeep/blob/master/docs/figures/Figure3.png?raw=true)

# References

Palluzzi F, Grassi M. **SEMgraph: An R Package for Causal Network Analysis of High-Throughput Data with Structural Equation Models**. 3 Jan 2022; arXiv:2103.08332.

[{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Mario Grassi. Author. Barbara Tarantino. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Grassi M, Tarantino B (2024). SEMdeep: Structural Equation Modeling Deep Neural Network Machine Learning. R package version 0.1.0, https://CRAN.R-project.org/package=SEMdeep.","code":"@Manual{,   title = {SEMdeep: Structural Equation Modeling with Deep Neural Network and Machine Learning},   author = {Mario Grassi and Barbara Tarantino},   year = {2024},   note = {R package version 0.1.0},   url = {https://CRAN.R-project.org/package=SEMdeep}, }"},{"path":"/index.html","id":"semdeep","dir":"","previous_headings":"","what":"Structural Equation Modeling with Deep Neural Network and Machine Learning","title":"Structural Equation Modeling with Deep Neural Network and Machine Learning","text":"Structural Equation Modeling Deep Neural Network Machine Learning SEMdeep train validate custom (data-driven) structural equation model (SEM) using layer-wise deep neural networks (DNNs) node-wise machine learning (ML) algorithms. SEMdeep comes following functionalities: Automated ML DNN model training based SEM network structures. Network plot representation interpretation diagram. Model performance evaluation regression classification metrics. Model variable importance computation Shapley (R2) values, Gradient (Connection) weight approach significance tests network inputs.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Structural Equation Modeling with Deep Neural Network and Machine Learning","text":"SEMdeep uses deep learning framework ‘torch’. torch package native R, ’s computationally efficient, need install Python API, DNNs can trained CPU, GPU MacOS GPUs. using ‘SEMdeep’ make sure current version ‘torch’ installed running: windows (Linux Mac). Windows distributions don’t Visual C++ runtime pre-installed, download [Microsoft] (https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170) VC_redist.x86.exe (R32) VC_redist.x86.exe (R64) install . GPU setup, problems installing torch package, check installation help torch developer. , latest stable version can installed CRAN: latest development version can installed GitHub:","code":"install.packages(\"torch\")  library(torch)  install_torch(reinstall = TRUE) install.packages(\"SEMdeep\") # install.packages(\"devtools\") devtools::install_github(\"BarbaraTarantino/SEMdeep\")"},{"path":"/index.html","id":"getting-help","dir":"","previous_headings":"","what":"Getting help","title":"Structural Equation Modeling with Deep Neural Network and Machine Learning","text":"full list SEMdeep functions examples available website .","code":""},{"path":"/reference/getConnectionWeight.html","id":null,"dir":"Reference","previous_headings":"","what":"Connection Weight Approach for neural network variable importance — getConnectionWeight","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"function computes product raw input-hidden hidden-output connection weights input output neuron sums products across hidden neurons, proposed Olden (2004).","code":""},{"path":"/reference/getConnectionWeight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"","code":"getConnectionWeight(object, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getConnectionWeight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"object neural network object SEMdnn() function. thr value threshold apply connection weights. NULL (default), threshold set thr=mean(abs(connection weights)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getConnectionWeight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"list od two object: () data.frame including connections together weights, (ii) DAG colored edges. abs(W) > thr W < 0, edge inhibited highlighted blue; otherwise, abs(W) > thr W > 0, edge activated highlighted red.","code":""},{"path":"/reference/getConnectionWeight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"neural network, connections inputs outputs represented connection weights neurons. importance values assigned input variable using Olden method units based directly summed product connection weights. amount direction link weights largely determine proportional contributions input variables neural network's prediction output. Input variables larger connection weights indicate higher intensities signal transfer therefore important prediction process. Positive connection weights represent excitatory effects neurons (raising intensity incoming signal) increase value predicted response, negative connection weights represent inhibitory effects neurons (reducing intensity incoming signal). weights change sign (e.g., positive negative) input-hidden hidden-output layers cancelling effect, vice versa weights sign synergistic effect. Note order map connection weights DAG edges, element-wise product, W*performed Olden's weights entered matrix, W(pxp) binary (1,0) adjacency matrix, (pxp) input DAG.","code":""},{"path":"/reference/getConnectionWeight.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"Olden, Julian & Jackson, Donald. (2002). Illuminating \"black box\": randomization approach understanding variable contributions artificial neural networks. Ecological Modelling. 154. 135-150. 10.1016/S0304-3800(02)00064-9. Olden, Julian. (2004). accurate comparison methods quantifying variable importance artificial neural networks using simulated data. Ecological Modelling. 178. 10.1016/S0304-3800(04)00156-5.","code":""},{"path":"/reference/getConnectionWeight.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getConnectionWeight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connection Weight Approach for neural network variable importance — getConnectionWeight","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data  dnn0 <- SEMdnn(ig, data, train=1:nrow(data), cowt = FALSE,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE)  res<- getConnectionWeight(dnn0, thr=NULL, verbose=TRUE) table(E(res$dag)$color) } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch train_l valid_l #> 32    32 0.26668      NA #>  #> 2 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.3184782      NA #>  #> 3 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.3307049      NA #>  #> 4 : z596 z4217   #>    epoch   train_l valid_l #> 32    32 0.3400184      NA #>  #> 5 : z1616   #>    epoch   train_l valid_l #> 32    32 0.3419062      NA #>  #> DNN solver ended normally after 736 iterations  #>  #>  logL: -42.77156  srmr: 0.1001332  #>   #>  #>     gray50       red2 royalblue3  #>         30          7          8  # }"},{"path":"/reference/getGradientWeight.html","id":null,"dir":"Reference","previous_headings":"","what":"Gradient Weight Approach for neural network variable importance — getGradientWeight","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"function computes gradient matrix, .e., average conditional effects input variables w.r.t neural network model, discussed Amesöder et al (2024).","code":""},{"path":"/reference/getGradientWeight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"","code":"getGradientWeight(object, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getGradientWeight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"object neural network object SEMdnn() function. thr threshold value apply gradient weights input nodes (variables). NULL (default), threshold set thr=mean(abs(gradient weights)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getGradientWeight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"list od two object: () data.frame including connections together weights, (ii) DAG colored edges. abs(W) > thr W < 0, edge inhibited highlighted blue; otherwise, abs(W) > thr W > 0, edge activated highlighted red.","code":""},{"path":"/reference/getGradientWeight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"partial derivatives method calculates derivative (gradient) output variable (y) respect input variable (x) evaluated observation (=1,...,n) training data. contribution input evaluated terms magnitude taking account connection weights activation functions, also values observation input variables. gradients variable observation, summary gradient calculated averaging observation units. Finally, average weights entered matrix, W(pxp) element-wise product binary (1,0) adjacency matrix, (pxp) input DAG, W*maps weights DAG edges. Note operations required compute partial derivatives time consuming compared methods Olden's (connection weight). computational time increases size neural network size data. Therefore, function uses progress bar check progress gradient evaluation per observation.","code":""},{"path":"/reference/getGradientWeight.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"Amesöder, C., Hartig, F. Pichler, M. (2024), ‘cito': R package training neural networks using ‘torch'. Ecography, 2024: e07143. https://doi.org/10.1111/ecog.07143","code":""},{"path":"/reference/getGradientWeight.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getGradientWeight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gradient Weight Approach for neural network variable importance — getGradientWeight","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data  dnn0 <- SEMdnn(ig, data, train=1:nrow(data), cowt = FALSE,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE)  res<- getGradientWeight(dnn0, thr=NULL, verbose=TRUE) table(E(res$dag)$color) } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch   train_l valid_l #> 32    32 0.2681521      NA #>  #> 2 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.3359614      NA #>  #> 3 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.3410804      NA #>  #> 4 : z596 z4217   #>    epoch   train_l valid_l #> 32    32 0.3820579      NA #>  #> 5 : z1616   #>    epoch   train_l valid_l #> 32    32 0.3302944      NA #>  #> DNN solver ended normally after 736 iterations  #>  #>  logL: -43.20641  srmr: 0.0979507  #>  #>    |                                                                               |                                                                      |   0%   |                                                                               |==============                                                        |  20%   |                                                                               |============================                                          |  40%   |                                                                               |==========================================                            |  60%   |                                                                               |========================================================              |  80%   |                                                                               |======================================================================| 100%  #>  #>     gray50       red2 royalblue3  #>         27          8         10  # }"},{"path":"/reference/getInputPvalue.html","id":null,"dir":"Reference","previous_headings":"","what":"Test for the significance of neural network inputs — getInputPvalue","title":"Test for the significance of neural network inputs — getInputPvalue","text":"function computes formal test significance neural network input nodes, based linear relationship observed output predicted values input variable, input variables maintained mean values, proposed Mohammadi (2018).","code":""},{"path":"/reference/getInputPvalue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test for the significance of neural network inputs — getInputPvalue","text":"","code":"getInputPvalue(object, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getInputPvalue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test for the significance of neural network inputs — getInputPvalue","text":"object neural network object SEMdnn() function. thr value threshold apply input p-values. thr=NULL (default), threshold set thr=0.05. verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getInputPvalue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test for the significance of neural network inputs — getInputPvalue","text":"list od two object: () data.frame including connections together p-values, (ii) DAG colored edges. p-values > thr t-test < 0, edge inhibited highlighted blue; otherwise, p-values > thr t-test > 0, edge activated highlighted red.","code":""},{"path":"/reference/getInputPvalue.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test for the significance of neural network inputs — getInputPvalue","text":"neural network arbitrary architecture trained, taking account factors like number neurons, hidden layers, activation function. , network's output simulated get predicted values output variable, fixing inputs (exception one nonconstant input variable) mean values; network’s predictions saved, input variable. last step, multiple regression analysis applied node-wise (mapping input DAG) observed output nodes predicted values input nodes explanatory variables. statistical significance coefficients evaluated standard t-student critical values, represent importance input variables.","code":""},{"path":"/reference/getInputPvalue.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Test for the significance of neural network inputs — getInputPvalue","text":"S. Mohammadi. new test significance neural network inputs. Neurocomputing 2018; 273: 304-322.","code":""},{"path":"/reference/getInputPvalue.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Test for the significance of neural network inputs — getInputPvalue","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getInputPvalue.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test for the significance of neural network inputs — getInputPvalue","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data  dnn0 <- SEMdnn(ig, data, train=1:nrow(data), cowt = FALSE,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE)  res<- getInputPvalue(dnn0, thr=NULL, verbose=TRUE) table(E(res$dag)$color) } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch   train_l valid_l #> 32    32 0.2739701      NA #>  #> 2 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.3321843      NA #>  #> 3 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.3062268      NA #>  #> 4 : z596 z4217   #>    epoch   train_l valid_l #> 32    32 0.3620388      NA #>  #> 5 : z1616   #>    epoch   train_l valid_l #> 32    32 0.3512816      NA #>  #> DNN solver ended normally after 736 iterations  #>  #>  logL: -43.22707  srmr: 0.0913046  #>   #>  #> gray50   red2  #>     27     18  # }"},{"path":"/reference/getShapleyR2.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute variable importance using Shapley (R2) values — getShapleyR2","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"function computes variable contributions individual predictions using Shapley values, method cooperative game theory variable values observation work together achieve prediction. addition, make variable contributions easily explainable, function decomposes entire model R-Squared (R2 coefficient determination) variable-level attributions variance (Redell, 2019).","code":""},{"path":"/reference/getShapleyR2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"","code":"getShapleyR2(object, newdata, thr = NULL, verbose = FALSE, ...)"},{"path":"/reference/getShapleyR2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"object model fitting object SEMml() function. newdata matrix containing new data rows corresponding subjects, columns variables. thr threshold value apply signed Shapley (R2) values. thr=NULL (default), threshold set thr=mean(Shapley(R2) values)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/getShapleyR2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"list od three object: () data.frame including connections together signed Shapley R-squred values; (ii) dag colored edges, abs(sign_R2) > thr highlighted red (sign_R2 > 0) blue (sign_R2 < 0); (ii) list individual Shapley values predictors variables per response variable.","code":""},{"path":"/reference/getShapleyR2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"Lundberg & Lee (2017) proposed unified approach local explainability (variable contribution single variable within single sample) global explainability (variable contribution entire model) applying fair distribution payoffs principles game theory put forth Shapley (1953). Now called SHAP (SHapley Additive exPlanations), suggested framework explains predictions ML models, input variables take place players, contribution particular prediction measured using Shapley values. Successively, Redell (2019) presented metric combines additive property Shapley values robustness R2 Gelman (2018) produce R2 variance decomposition accurately captures contribution variable explanatory power model. Additionally, use signed R2, order denote regulation connections line linear SEM, since edges DAG indicate node regulation (activation, positive; inhibition, negative). recovered edge using sign(beta), .e., sign coefficient estimates linear model (lm) fitting output node input nodes, suggested Joseph (2019). noted order ascertain local significance node regulation respect DAG, Shapley decomposition R-squared (R2) value can employed outcome node (r=1,...,R) averaging R2 indices input nodes. Shapley values computed using shapr package implements extended version Kernel SHAP method approximating Shapley values dependence features taken account. operations necessary compute kernel SHAP values inherently time-consuming, computational time increasing proportion number predictor variables number observations. Therefore, function uses progress bar check progress kernel SHAP evaluation per observation.","code":""},{"path":"/reference/getShapleyR2.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"Shapley, L. (1953) Value n-Person Games. : Kuhn, H. Tucker, ., Eds., Contributions Theory Games II, Princeton University Press, Princeton, 307-317. Scott M. Lundberg, Su-Lee. (2017). unified approach interpreting model predictions. Proceedings 31st International Conference Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 4768–4777. Redell, N. (2019). Shapley Decomposition R-Squared Machine Learning Models. arXiv: Methodology. Gelman, ., Goodrich, B., Gabry, J., & Vehtari, . (2019). R-squared Bayesian Regression Models. American Statistician, 73(3), 307–309. Joseph, . Parametric inference universal function approximators (2019). Bank England working papers 784, Bank England, revised 22 Jul 2020.","code":""},{"path":"/reference/getShapleyR2.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/getShapleyR2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute variable importance using Shapley (R2) values — getShapleyR2","text":"","code":"# \\donttest{ # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done.  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  rf0<- SEMml(ig, data, train=train, algo=\"rf\", vimp=FALSE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  RF solver ended normally after 23 iterations  #>  #>  logL: -33.22824  srmr: 0.0859545  #>   res<- getShapleyR2(rf0, data[-train, ], thr=NULL, verbose=TRUE) #>    |                                                                               |                                                                      |   0%   |                                                                               |===                                                                   |   4%   |                                                                               |======                                                                |   9%   |                                                                               |=========                                                             |  13%   |                                                                               |============                                                          |  17%   |                                                                               |===============                                                       |  22%   |                                                                               |==================                                                    |  26%   |                                                                               |=====================                                                 |  30%   |                                                                               |========================                                              |  35%   |                                                                               |===========================                                           |  39%   |                                                                               |==============================                                        |  43%   |                                                                               |=================================                                     |  48%   |                                                                               |=====================================                                 |  52%   |                                                                               |========================================                              |  57%   |                                                                               |===========================================                           |  61%   |                                                                               |==============================================                        |  65%   |                                                                               |=================================================                     |  70%   |                                                                               |====================================================                  |  74%   |                                                                               |=======================================================               |  78%   |                                                                               |==========================================================            |  83%   |                                                                               |=============================================================         |  87%   |                                                                               |================================================================      |  91%   |                                                                               |===================================================================   |  96%   |                                                                               |======================================================================| 100%  table(E(res$dag)$color) #>  #>     gray50       red2 royalblue3  #>         27          8         10   # average shapley R2 across response variables R2<- abs(res$est[,4]) Y<- res$est[,1] R2Y<- aggregate(R2~Y,data=data.frame(R2,Y),FUN=\"mean\") PE<- predict(rf0, data[-train, ])$PE cbind(R2Y=R2Y[,2],PEY=PE[-1]) #>              R2Y       PEY #> 10452 0.30137196 1.5751174 #> 1432  0.13790902 0.8407252 #> 1616  0.05305108 1.2992863 #> 4217  0.24696402 1.4710206 #> 4741  0.04265047 0.9542770 #> 4744  0.04695295 0.9797583 #> 4747  0.04979782 0.7995575 #> 54205 0.05470003 1.2948457 #> 5530  0.42754031 1.3477223 #> 5532  0.38832943 1.3374130 #> 5533  0.35283860 1.7462924 #> 5534  0.49553199 1.3393998 #> 5535  0.58557539 0.9579940 #> 5600  0.14279066 0.5885530 #> 5603  0.00000000 1.1292464 #> 5606  0.29351235 1.1417869 #> 5608  0.51480155 1.1304838 #> 596   0.20377932 1.2727327 #> 6300  0.19044710 0.6232077 #> 79139 0.49565753 0.8256068 #> 836   0.57852045 0.8613661 #> 84134 0.46729156 1.1949609 #> 842   0.17292050 0.7906486 mean(R2) # total average R2 #> [1] 0.1713679 PE[1]    # total MSE #>     amse  #> 1.108783  # }"},{"path":"/reference/mapGraph.html","id":null,"dir":"Reference","previous_headings":"","what":"Map additional variables (nodes) to a graph object — mapGraph","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"function insert additional nodes graph object. Among node types, additional source sink nodes can added. Regarding former, source nodes can represent: () data variables; (ii) group variable; (iii) Latent Variables (LV). latter, outcome variable, representing prediction interest, can added. Moreover, mapGraph() can also create new graph object starting compact symbolic formula.","code":""},{"path":"/reference/mapGraph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"","code":"mapGraph(graph, type = \"outcome\", LV = NULL, f = NULL, ...)"},{"path":"/reference/mapGraph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"graph igraph object. type character value specifying type mapping. Five types can specified. type = \"source\" specified, additional source node () added graph. type = \"group\", additional group source node added. type = \"outcome\" (default), prediction sink node mapped graph. type = \"LV\", LV source node included (number LV depends LV argument). type = \"clusterLV\", series clusters data computed different LV source node added separately cluster. LV number LV source nodes add graph. argument needs specified type = \"LV\". type = \"clusterLV\" LV number defined internally equal number clusters. (default = NULL). f formula object (default = NULL). new graph object created according specified formula object. ... Currently ignored.","code":""},{"path":"/reference/mapGraph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"mapGraph returns invisibly graphical object mapped node variables.","code":""},{"path":"/reference/mapGraph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/mapGraph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Map additional variables (nodes) to a graph object — mapGraph","text":"","code":"# Load Amyotrophic Lateral Sclerosis (ALS) ig<- alsData$graph; gplot(ig)   # ... map source nodes to ALS graph  ig1 <- mapGraph(ig, type = \"source\"); gplot(ig1, l=\"dot\")   # ... map group source node to ALS graph  ig2 <- mapGraph(ig, type = \"group\"); gplot(ig2, l=\"fdp\")   # ... map outcome sink to ALS graph  ig3 <- mapGraph(ig, type = \"outcome\"); gplot(ig3, l=\"dot\")   # ... map LV source nodes to ALS graph  ig4 <- mapGraph(ig, type = \"LV\", LV = 3); gplot(ig4, l=\"fdp\")   # ... map LV source nodes to the clusters of ALS graph  ig5 <- mapGraph(ig, type = \"clusterLV\"); gplot(ig5, l=\"dot\") #> modularity = 0.5588502  #>  #> Community sizes #>  3  1  4  2  #>  4  8  9 11  #>    # ... create a new graph with the formula variables formula <- as.formula(\"z4747 ~ z1432 + z5603 + z5630\") ig6 <- mapGraph(f=formula); gplot(ig6)"},{"path":"/reference/nplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a plot for a neural network model — nplot","title":"Create a plot for a neural network model — nplot","text":"function draws neural network plot neural interpretation diagram using plotnet function NeuralNetTools R package.","code":""},{"path":"/reference/nplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a plot for a neural network model — nplot","text":"","code":"nplot(dnn.fit, bias = FALSE, ...)"},{"path":"/reference/nplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a plot for a neural network model — nplot","text":"dnn.fit neural network model cito R package. bias logical value, indicating whether draw biases layers (default = FALSE). ... Currently ignored.","code":""},{"path":"/reference/nplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a plot for a neural network model — nplot","text":"nplot returns invisibly graphical object representing neural network architecture NeuralNetTools.","code":""},{"path":"/reference/nplot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a plot for a neural network model — nplot","text":"induced subgraph input graph mapped data variables. Based estimated connection weights, connection weight W > 0, connection activated highlighted red; W < 0, connection inhibited highlighted blue.","code":""},{"path":"/reference/nplot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a plot for a neural network model — nplot","text":"Beck, M.W. 2018. NeuralNetTools: Visualization Analysis Tools Neural Networks. Journal Statistical Software. 85(11):1-20.","code":""},{"path":"/reference/nplot.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create a plot for a neural network model — nplot","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/nplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a plot for a neural network model — nplot","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data  dnn0 <- SEMdnn(ig, data, train=1:nrow(data), grad = FALSE,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE)   for (j in 1:length(dnn0$model)) {     nplot(dnn0$model[[j]])    Sys.sleep(5)  } } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch   train_l valid_l #> 32    32 0.2899463      NA #>  #> 2 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.3224069      NA #>  #> 3 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.3020006      NA #>  #> 4 : z596 z4217   #>    epoch   train_l valid_l #> 32    32 0.3790084      NA #>  #> 5 : z1616   #>    epoch   train_l valid_l #> 32    32 0.3122744      NA #>  #> DNN solver ended normally after 736 iterations  #>  #>  logL: -43.0303  srmr: 0.0989378  #>       # }"},{"path":"/reference/performance.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction performance evaluation utility — performance","title":"Prediction performance evaluation utility — performance","text":"function able calculate series binary classification evaluation statistics given () two vectors: one true target variable values, predicted target variable values (ii) confusion matrix counts False Positives (FP), True Positives (TP), True Negatives (TN), False Negatives (FN). user can specify desired set metrics compute: () precision, recall, f1 score Matthews Correlation Coefficient (mcc) (ii) specificity, sensitivity, accuracy mcc.","code":""},{"path":"/reference/performance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction performance evaluation utility — performance","text":"","code":"performance(yobs, yhat, CT = NULL, thr = 0, F1 = TRUE, verbose = FALSE, ...)"},{"path":"/reference/performance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction performance evaluation utility — performance","text":"yobs binary vector true target variable values. yhat continuous vector predicted target variable values. CT optional confusion matrix dimension 2x2 containing counts FP, TP, TN, FN. thr numerical value indicating threshold converting yhat continuous vector binary vector. yhat vector ranges -1 1, user can specify thr = 0 (default); yhat ranges 0 1, user can specify thr = 0.5. F1 logical value. TRUE (default), precision (pre), recall (rec), f1 mcc computed. Otherwise, FALSE, specificity (sp), sensitivity (se), accuracy (acc) mcc obtained. verbose logical value. FALSE (default), density plots yhat per group plotted screen. ... Currently ignored.","code":""},{"path":"/reference/performance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction performance evaluation utility — performance","text":"data.frame classification evaluation statistics returned.","code":""},{"path":"/reference/performance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prediction performance evaluation utility — performance","text":"#' Suppose 2x2 table notation formulas used : $$se = /(+C)$$ $$sp = D/(B+D)$$ $$acc = (+D)/(+B+C+D)$$ $$pre = /(+B)$$ $$rec = /(+C)$$ $$F1 = (2*pre*rec)/(pre+rec)$$ $$mcc = (*D - B*C)/sqrt((+B)*(+C)*(D+B)*(D+C))$$","code":""},{"path":"/reference/performance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prediction performance evaluation utility — performance","text":"Sammut, C. & Webb, G. . (eds.) (2017). Encyclopedia Machine Learning Data Mining. New York: Springer. ISBN: 978-1-4899-7685-7 Chicco, D., Jurman, G. (2020) advantages Matthews correlation coefficient (MCC) F1 score accuracy binary classification evaluation. BMC Genomics 21, 6.","code":""},{"path":"/reference/performance.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prediction performance evaluation utility — performance","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/performance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prediction performance evaluation utility — performance","text":"","code":"# \\donttest{ # Load Amyotrophic Lateral Sclerosis (ALS) data<- alsData$exprs; dim(data) #> [1] 160 318 data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- alsData$group; table (group) #> group #>   0   1  #>  21 139  ig<- alsData$graph; gplot(ig)   #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  #...with a binary outcome (1=case, 0=control) ig1<- mapGraph(ig, type = \"outcome\"); gplot(ig1)  outcome<- group; table(outcome) #> outcome #>   0   1  #>  21 139  data1<- cbind(outcome, data); data1[1:5,1:5] #>      outcome        207         208      10000       284 #> ALS2       1 -1.8273895 -0.45307006 -0.1360061 0.4530701 #> ALS3       1 -2.5616910 -0.96201413  0.3160400 0.6762093 #> ALS4       1 -0.8003346  0.82216031 -1.1521227 0.5613048 #> ALS5       1 -2.1342965 -0.98709115  1.1521227 0.5064807 #> ALS6       1 -2.0111279  0.02393297  0.5987578 0.1360061  res <- SEMml(ig1, data1, train, algo=\"rf\") #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #> 24 : zoutcome  #>  #>  RF solver ended normally after 24 iterations  #>  #>  logL: -34.36475  srmr: 0.0854561  #>  mse <- predict(res, data1[-train, ]) yobs<- group[-train] yhat<- mse$Yhat[ ,\"outcome\"] #yprob<- exp(yhat)/(1+exp(yhat)) #performance(yobs, yprob, thr=0.5)  # ... evaluate predictive performance (sp, se, acc, mcc) performance(yobs, yhat, thr=0, F1=FALSE) #>     ypred #> yobs  0  1 #>    0  5  1 #>    1 13 61 #>  #>          sp        se   acc       mcc #> 1 0.8333333 0.8243243 0.825 0.4148196  # ... evaluate predictive performance (pre, rec, f1, mcc) performance(yobs, yhat, thr=0, F1=TRUE) #>     ypred #> yobs  0  1 #>    0  5  1 #>    1 13 61 #>  #>        pre       rec        f1       mcc #> 1 0.983871 0.8243243 0.8970588 0.4148196  #... with confusion matrix table as input ypred<- ifelse(yhat < 0, 0, 1) performance(CT=table(yobs, ypred), F1=TRUE) #>     ypred #> yobs  0  1 #>    0  6  0 #>    1 19 55 #>  #>   pre       rec        f1       mcc #> 1   1 0.7432432 0.8527132 0.4223486  #...with density plots of yhat per group old.par <- par(no.readonly = TRUE) performance(yobs, yhat, thr=0, F1=FALSE, verbose = TRUE) #>     ypred #> yobs  0  1 #>    0  5  1 #>    1 13 61 #>   #>          sp        se   acc       mcc #> 1 0.8333333 0.8243243 0.825 0.4148196 par(old.par) # }"},{"path":"/reference/predict.DNN.html","id":null,"dir":"Reference","previous_headings":"","what":"SEM-based out-of-sample prediction using layer-wise DNN — predict.DNN","title":"SEM-based out-of-sample prediction using layer-wise DNN — predict.DNN","text":"Predict method DNN objects.","code":""},{"path":"/reference/predict.DNN.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SEM-based out-of-sample prediction using layer-wise DNN — predict.DNN","text":"","code":"# S3 method for class 'DNN' predict(object, newdata, verbose = FALSE, ...)"},{"path":"/reference/predict.DNN.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SEM-based out-of-sample prediction using layer-wise DNN — predict.DNN","text":"object model fitting object SEMdnn() function. newdata matrix containing new data rows corresponding subjects, columns variables. verbose Print predicted --sample MSE values (default = FALSE). ... Currently ignored.","code":""},{"path":"/reference/predict.DNN.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SEM-based out-of-sample prediction using layer-wise DNN — predict.DNN","text":"list 2 objects: \"PE\", vector prediction error equal Mean Squared Error (MSE) --bag prediction. first value PE AMSE, average (sink mediators) graph nodes. \"Yhat\", matrix continuous predicted values graph nodes (excluding source nodes) based --bag samples.","code":""},{"path":"/reference/predict.DNN.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SEM-based out-of-sample prediction using layer-wise DNN — predict.DNN","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/predict.DNN.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SEM-based out-of-sample prediction using layer-wise DNN — predict.DNN","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # Load Amyotrophic Lateral Sclerosis (ALS) data<- alsData$exprs; dim(data) data<- transformData(data)$data ig<- alsData$graph; gplot(ig) group<- alsData$group   #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  start<- Sys.time() dnn0 <- SEMdnn(ig, data, train, cowt = FALSE, thr = NULL,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE) end<- Sys.time() print(end-start) mse0 <- predict(dnn0, data[-train, ], verbose=TRUE)  # SEMrun vs. SEMdnn MSE comparison sem0 <- SEMrun(ig, data[train, ], SE=\"none\", limit=1000) mse0 <- predict(sem0, data[-train,], verbose=TRUE)  #...with a binary outcome (1=case, 0=control)  ig1<- mapGraph(ig, type=\"outcome\"); gplot(ig1) outcome<- ifelse(group == 0, -1, 1); table(outcome) data1<- cbind(outcome, data); data1[1:5,1:5]  start<- Sys.time() dnn1 <- SEMdnn(ig1, data1, train, cowt = TRUE, thr = NULL,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE) end<- Sys.time() print(end-start)  mse1 <- predict(dnn1, data1[-train, ]) yobs <- group[-train] yhat <- mse1$Yhat[ ,\"outcome\"] performance(yobs, yhat, thr=0, F1=FALSE) } #> Conducting the nonparanormal transformation via shrunkun ECDF...done.  #> 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch   train_l valid_l #> 32    32 0.2418655      NA #>  #> 2 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.2461891      NA #>  #> 3 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.2459619      NA #>  #> 4 : z596 z4217   #>    epoch   train_l valid_l #> 32    32 0.3024978      NA #>  #> 5 : z1616   #>    epoch  train_l valid_l #> 32    32 0.275672      NA #>  #> DNN solver ended normally after 736 iterations  #>  #>  logL: -33.13593  srmr: 0.0964108  #>  #> Time difference of 5.972645 secs #>      amse     10452     84134       836      4747      4741      4744     79139  #> 0.5402869 0.4844612 0.3641247 0.4481633 0.4016256 0.6602336 0.6948710 0.5367122  #>      5530      5532      5533      5534      5535       842      1432      5600  #> 0.3129730 0.2536711 1.0265263 0.2136801 0.6478210 0.6604281 0.6117236 0.4225854  #>      5603      6300     54205      5606      5608       596      4217      1616  #> 0.7649073 0.4719116 0.7138801 0.4396509 0.5012569 0.7151702 0.4927121 0.5875085  #> NLMINB solver ended normally after 1 iterations  #>  #> deviance/df: 6.263349  srmr: 0.3041398  #>   #>      amse     10452     84134       836      4747      4741      4744     79139  #> 0.8973153 1.1160997 0.8591412 0.6957059 0.7435081 1.0773819 1.0733166 0.6766548  #>      5530      5532      5533      5534      5535       842      1432      5600  #> 0.9041601 0.8519291 1.2303948 0.8115211 0.6638488 0.7160223 0.8122749 0.5717884  #>      5603      6300     54205      5606      5608       596      4217      1616  #> 1.0343893 0.6118950 1.1161717 0.8886350 0.9082540 1.0675990 1.0817833 1.1257768   #> 1 : zoutcome   #>    epoch     train_l valid_l #> 32    32 0.005631989      NA #>  #> 2 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch  train_l valid_l #> 32    32 0.225479      NA #>  #> 3 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.2600558      NA #>  #> 4 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.2284973      NA #>  #> 5 : z596 z4217   #>    epoch   train_l valid_l #> 32    32 0.2676658      NA #>  #> 6 : z1616   #>    epoch   train_l valid_l #> 32    32 0.2420339      NA #>  #> DNN solver ended normally after 768 iterations  #>  #>  logL: -31.75535  srmr: 0.0875989  #>  #> Time difference of 6.571721 secs #>     ypred #> yobs  0  1 #>    0  5  1 #>    1  8 66 #>  #>          sp        se    acc      mcc #> 1 0.8333333 0.8918919 0.8875 0.517792 # }"},{"path":"/reference/predict.ML.html","id":null,"dir":"Reference","previous_headings":"","what":"SEM-based out-of-sample prediction using node-wise ML — predict.ML","title":"SEM-based out-of-sample prediction using node-wise ML — predict.ML","text":"Predict method ML objects.","code":""},{"path":"/reference/predict.ML.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SEM-based out-of-sample prediction using node-wise ML — predict.ML","text":"","code":"# S3 method for class 'ML' predict(object, newdata, verbose = FALSE, ...)"},{"path":"/reference/predict.ML.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SEM-based out-of-sample prediction using node-wise ML — predict.ML","text":"object model fitting object SEMml() function. newdata matrix containing new data rows corresponding subjects, columns variables. verbose Print predicted --sample MSE values (default = FALSE). ... Currently ignored.","code":""},{"path":"/reference/predict.ML.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SEM-based out-of-sample prediction using node-wise ML — predict.ML","text":"list 2 objects: \"PE\", vector prediction error equal Mean Squared Error (MSE) --bag prediction. first value PE AMSE, average (sink mediators) graph nodes. \"Yhat\", matrix continuous predicted values graph nodes (excluding source nodes) based --bag samples.","code":""},{"path":"/reference/predict.ML.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SEM-based out-of-sample prediction using node-wise ML — predict.ML","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/predict.ML.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SEM-based out-of-sample prediction using node-wise ML — predict.ML","text":"","code":"# \\donttest{ # Load Amyotrophic Lateral Sclerosis (ALS) data<- alsData$exprs; dim(data) #> [1] 160 318 data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. ig<- alsData$graph; gplot(ig)   #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  start<- Sys.time() # ... rf res1<- SEMml(ig, data, train, algo=\"rf\", vimp=FALSE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  RF solver ended normally after 23 iterations  #>  #>  logL: -33.22824  srmr: 0.0859545  #>  mse1<- predict(res1, data[-train, ], verbose=TRUE) #>      amse     10452      1432      1616      4217      4741      4744      4747  #> 1.1087827 1.5751174 0.8407252 1.2992863 1.4710206 0.9542770 0.9797583 0.7995575  #>     54205      5530      5532      5533      5534      5535      5600      5603  #> 1.2948457 1.3477223 1.3374130 1.7462924 1.3393998 0.9579940 0.5885530 1.1292464  #>      5606      5608       596      6300     79139       836     84134       842  #> 1.1417869 1.1304838 1.2727327 0.6232077 0.8256068 0.8613661 1.1949609 0.7906486   # ... xgb res2<- SEMml(ig, data, train, algo=\"xgb\", vimp=FALSE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  XGB solver ended normally after 23 iterations  #>  #>  logL: 70.10035  srmr: 0.0014393  #>  mse2<- predict(res2, data[-train, ], verbose=TRUE) #>      amse     10452      1432      1616      4217      4741      4744      4747  #> 1.5485977 2.3224700 1.0438189 2.0584053 2.1367318 0.9796140 1.1430960 0.7703339  #>     54205      5530      5532      5533      5534      5535      5600      5603  #> 1.4605175 2.0228881 2.0250110 2.2691126 2.0941554 1.5672913 0.8313812 1.2909388  #>      5606      5608       596      6300     79139       836     84134       842  #> 1.6402649 1.5021883 2.0930200 0.9203514 1.2559263 1.2960971 1.9018098 0.9923247   # ... nn res3<- SEMml(ig, data, train, algo=\"nn\", vimp=FALSE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  NN solver ended normally after 23 iterations  #>  #>  logL: -36.07395  srmr: 0.194907  #>  mse3<- predict(res3, data[-train, ], verbose=TRUE) #>      amse     10452      1432      1616      4217      4741      4744      4747  #> 1.6357984 1.5177940 1.7251374 2.0771794 1.2273523 2.7357480 4.8539956 2.7193534  #>     54205      5530      5532      5533      5534      5535      5600      5603  #> 2.4691147 0.9825952 1.2431796 1.6832744 1.3556345 0.8070092 1.7176961 2.1512864  #>      5606      5608       596      6300     79139       836     84134       842  #> 0.9625083 1.1228321 1.3024315 0.9162324 1.1671646 0.7527348 1.0170158 1.1160940   # ... gam res4<- SEMml(ig, data, train, algo=\"gam\", vimp=FALSE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  GAM solver ended normally after 23 iterations  #>  #>  logL: -46.77283  srmr: 0.3819281  #>  mse4<- predict(res4, data[-train, ], verbose=TRUE) #>      amse     10452      1432      1616      4217      4741      4744      4747  #> 0.9121870 1.1160997 0.8111562 1.1825384 1.0817833 1.0583429 1.1415647 0.6957230  #>     54205      5530      5532      5533      5534      5535      5600      5603  #> 1.1719439 0.9052094 0.8304518 1.3167643 0.8457815 0.7351789 0.5526870 1.0343893  #>      5606      5608       596      6300     79139       836     84134       842  #> 0.8886350 1.0385301 1.0676148 0.5683706 0.6435413 0.6957059 0.8558520 0.7424376   # ... sem res5<- SEMml(ig, data, train, algo=\"sem\", vimp=FALSE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  SEM solver ended normally after 23 iterations  #>  #>  logL: -47.90891  srmr: 0.3040025  #>  mse5<- predict(res5, data[-train, ], verbose=TRUE) #>      amse     10452      1432      1616      4217      4741      4744      4747  #> 0.8973153 1.1160997 0.8122749 1.1257768 1.0817833 1.0773819 1.0733166 0.7435081  #>     54205      5530      5532      5533      5534      5535      5600      5603  #> 1.1161717 0.9041601 0.8519291 1.2303948 0.8115211 0.6638488 0.5717884 1.0343893  #>      5606      5608       596      6300     79139       836     84134       842  #> 0.8886350 0.9082540 1.0675990 0.6118950 0.6766548 0.6957059 0.8591412 0.7160223  end<- Sys.time() print(end-start) #> Time difference of 4.399956 secs # }"},{"path":"/reference/predict.SEM.html","id":null,"dir":"Reference","previous_headings":"","what":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"Given values (observed) x-variables SEM, function may used predict values (observed) y-variables. predictive procedure consists two steps: (1) construction topological layer (TL) ordering input graph; (2) prediction node y values layer, nodes included previous layers act predictors x.","code":""},{"path":"/reference/predict.SEM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"","code":"# S3 method for class 'SEM' predict(object, newdata, verbose = FALSE, ...)"},{"path":"/reference/predict.SEM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"object object, created function SEMrun() argument fit set fit = 0 fit = 1. newdata matrix new data, rows corresponding subjects, columns variables. object$fit model group variable (fit = 1), first column newdata must new group binary vector (0=control, 1=case). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/predict.SEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"list 2 objects: \"PE\", vector prediction error equal Mean Squared Error (MSE) --bag prediction. first value PE AMSE, average (sink mediators) graph nodes. \"Yhat\", matrix continuous predicted values graph nodes (excluding source nodes) based --bag samples.","code":""},{"path":"/reference/predict.SEM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"function first creates layer-based structure input graph. , SEM-based predictive approach (Rooij et al., 2022) used produce predictions accounting graph structure organised topological layers, j=1,...,L. iteration, response variables y nodes j layer predictors x nodes belonging previous j-1 layers. Predictions (y given x) based (joint y x) model-implied variance-covariance (Sigma) matrix mean vector (Mu) fitted SEM, standard expression conditional mean multivariate normal distribution. Thus, layer structure described SEM taken consideration, differs ordinary least squares (OLS) regression.","code":""},{"path":"/reference/predict.SEM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"de Rooij M, Karch JD, Fokkema M, Bakk Z, Pratiwi BC, Kelderman H (2023). SEM-Based --Sample Predictions, Structural Equation Modeling: Multidisciplinary Journal, 30:1, 132-148 <https://doi.org/10.1080/10705511.2022.2061494> Grassi M, Palluzzi F, Tarantino B (2022). SEMgraph: R Package Causal Network Analysis High-Throughput Data Structural Equation Models. Bioinformatics, 38 (20), 4829–4830 <https://doi.org/10.1093/bioinformatics/btac567>","code":""},{"path":"/reference/predict.SEM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/predict.SEM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SEM-based out-of-sample prediction using layer-wise ordering — predict.SEM","text":"","code":"# load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- alsData$group  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  # SEM fitting #sem0<- SEMrun(ig, data[train,], algo=\"lavaan\", SE=\"none\") #sem0<- SEMrun(ig, data[train,], algo=\"ricf\", n_rep=0) sem0<- SEMrun(ig, data[train,], SE=\"none\", limit=1000) #> NLMINB solver ended normally after 1 iterations  #>  #> deviance/df: 6.263349  srmr: 0.3041398  #>   # predictors, source+mediator; outcomes, mediator+sink  res0<- predict(sem0, newdata=data[-train,])  print(res0$PE) #>      amse     10452     84134       836      4747      4741      4744     79139  #> 0.8973153 1.1160997 0.8591412 0.6957059 0.7435081 1.0773819 1.0733166 0.6766548  #>      5530      5532      5533      5534      5535       842      1432      5600  #> 0.9041601 0.8519291 1.2303948 0.8115211 0.6638488 0.7160223 0.8122749 0.5717884  #>      5603      6300     54205      5606      5608       596      4217      1616  #> 1.0343893 0.6118950 1.1161717 0.8886350 0.9082540 1.0675990 1.0817833 1.1257768   # SEM fitting #sem1<- SEMrun(ig, data[train,], group[train], algo=\"lavaan\", SE=\"none\") #sem1<- SEMrun(ig, data[train,], group[train], algo=\"ricf\", n_rep=0) sem1<- SEMrun(ig, data[train,], group[train], SE=\"none\", limit=1000) #> NLMINB solver ended normally after 6 iterations  #>  #> deviance/df: 6.211221  srmr: 0.2861178  #>   # predictors, source+mediator+group; outcomes, source+mediator+sink  res1<- predict(sem1, newdata=cbind(group,data)[-train,])  print(res1$PE) #>      amse     10452     84134       836      4747      4741      4744     79139  #> 0.8963810 1.1258657 0.9073936 0.6861219 0.7001557 1.0070725 1.1045920 0.6652474  #>      5530      5532      5533      5534      5535       842      1432      5600  #> 0.8679917 0.7365461 1.2300132 0.6637344 0.6675348 0.7262147 0.8576185 0.6667256  #>      5603      6300      5630     54205       317      5606      5608       581  #> 1.1063933 0.6068862 1.0886300 1.0784325 0.7720406 0.8841224 0.8326926 1.0285427  #>       572       596       598      4217      6647      1616      7132      7133  #> 0.7379857 1.0832421 0.8036031 1.0731173 1.0383387 1.1300782 0.8195168 1.0913615   # \\donttest{ #...with a binary outcome (1=case, 0=control)  ig1<- mapGraph(ig, type=\"outcome\"); gplot(ig1)  outcome<- ifelse(group == 0, -1, 1); table(outcome) #> outcome #>  -1   1  #>  21 139  data1<- cbind(outcome, data); data1[1:5,1:5] #>      outcome        207         208      10000       284 #> ALS2       1 -1.8273895 -0.45307006 -0.1360061 0.4530701 #> ALS3       1 -2.5616910 -0.96201413  0.3160400 0.6762093 #> ALS4       1 -0.8003346  0.82216031 -1.1521227 0.5613048 #> ALS5       1 -2.1342965 -0.98709115  1.1521227 0.5064807 #> ALS6       1 -2.0111279  0.02393297  0.5987578 0.1360061  sem10 <- SEMrun(ig1, data1[train,], SE=\"none\", limit=1000) #> NLMINB solver ended normally after 1 iterations  #>  #> deviance/df: 6.122632  srmr: 0.3101125  #>  res10<- predict(sem10, newdata=data1[-train,], verbose=TRUE)   #>      amse   outcome     10452     84134       836      4747      4741      4744  #> 0.8828829 0.5509374 1.1160997 0.8591412 0.6957059 0.7435081 1.0773819 1.0733166  #>     79139      5530      5532      5533      5534      5535       842      1432  #> 0.6766548 0.9041601 0.8519291 1.2303948 0.8115211 0.6638488 0.7160223 0.8122749  #>      5600      5603      6300     54205      5606      5608       596      4217  #> 0.5717884 1.0343893 0.6118950 1.1161717 0.8886350 0.9082540 1.0675990 1.0817833  #>      1616  #> 1.1257768   yobs<- group[-train] yhat<- res10$Yhat[,\"outcome\"] performance(yobs, yhat) #>     ypred #> yobs  0  1 #>    0  3  3 #>    1 14 60 #>  #>        pre       rec        f1       mcc #> 1 0.952381 0.8108108 0.8759124 0.2001211  #...with predictors, source nodes; outcomes, sink nodes ig2<- mapGraph(ig, type= \"source\"); gplot(ig2)   sem02 <- SEMrun(ig2, data[train,], SE=\"none\", limit=1000) #> NLMINB solver ended normally after 1 iterations  #>  #> deviance/df: 10.16978  srmr: 0.1295992  #>  res02<- predict(sem02, newdata=data[-train,], verbose=TRUE)   #>      amse     10452     84134       836      4747      4741      4744     79139  #> 0.7550791 0.6146408 0.5114778 0.6960717 0.8871358 1.1343159 1.0664702 0.5043968  #>      5530      5532      5533      5534      5535  #> 0.6128182 0.6257037 1.1708051 0.6069758 0.6301368  #print(res02$PE)  #...with 10-iterations of 10-fold cross-validation samples  res<- NULL for (r in 1:10) {   set.seed(r)   cat(\"rep = \", r, \"\\n\")   idx <- SEMdeep:::createFolds(y=data[,1], k=10)   for (k in 1:10) {    cat(\"  k-fold = \", k, \"\\n\")    semr<- SEMdeep:::quiet(SEMrun(ig, data, SE=\"none\", limit=1000))    resr<- predict(semr, newdata=data[-idx[[k]], ])    res<- rbind(res, resr$PE)   } } #> rep =  1  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  2  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  3  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  4  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  5  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  6  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  7  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  8  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  9  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #> rep =  10  #>   k-fold =  1  #>   k-fold =  2  #>   k-fold =  3  #>   k-fold =  4  #>   k-fold =  5  #>   k-fold =  6  #>   k-fold =  7  #>   k-fold =  8  #>   k-fold =  9  #>   k-fold =  10  #res apply(res, 2, mean) #>      amse     10452     84134       836      4747      4741      4744     79139  #> 0.8203834 0.9924008 0.9373265 0.7580843 0.5039775 0.7884417 0.8785680 0.7226705  #>      5530      5532      5533      5534      5535       842      1432      5600  #> 0.8494899 0.7238154 0.9889105 0.8135880 0.7818244 0.5714233 0.7531485 0.8071404  #>      5603      6300     54205      5606      5608       596      4217      1616  #> 0.9521244 0.7174839 0.7012493 0.9636981 0.7552133 0.9749872 0.9882541 0.9449986  # }"},{"path":"/reference/SEMdnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"function builds topological layer (TL) ordering input graph fit series Deep Neural Networks (DNN) models, nodes one layer act response variables (output) y nodes sucessive layers act predictors (input) x. fit uses dnn function cito R package, based deep learning framework 'torch'. torch package native R, computationally efficient installation simple, need install Python API. order install torch please follow steps: install.packages(\"torch\") library(torch) install_torch(reinstall = TRUE) problems installing torch package, check installation help torch developer.","code":""},{"path":"/reference/SEMdnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"","code":"SEMdnn(   graph,   data,   train = NULL,   cowt = FALSE,   thr = NULL,   loss = \"mse\",   hidden = c(10L, 10L, 10L),   link = \"relu\",   validation = 0,   bias = TRUE,   lambda = 0,   alpha = 0.5,   dropout = 0,   optimizer = \"adam\",   lr = 0.01,   epochs = 100,   device = \"cpu\",   early_stopping = FALSE,   verbose = TRUE,   ... )"},{"path":"/reference/SEMdnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"graph igraph object. data matrix rows corresponding subjects, columns graph nodes (variables). train numeric vector specifying row indices corresponding train dataset. cowt logical value. cowt=TRUE connection weights input node (variables) computing (default = FALSE). thr numerical value indicating threshold apply absolute values connection matrix color graph (default = NULL). loss character value specifying loss network optimized. user can specify: () \"mse\" (mean squared error), \"mae\" (mean absolute error), \"gaussian\" (normal likelihood), regression problems; (b) \"poisson\" (poisson likelihood), \"nbinom\" (negative binomial likelihood) regression count data; (c) \"binomial\" (binomial likelihood) binary classification problems; (d) \"softmax\" \"cross-entropy\" multi-class classification (default = \"mse\"). hidden hidden units layers; number layers corresponds length hidden units. default, hidden = c(10L, 10L, 10L). link character value describing activation function use, might single length vector many activation functions assigned layer (default = \"relu\"). validation numerical value indicating proportion data set used validation set (randomly selected, default = 0). bias logical vector, indicating whether employ biases layers (bias = TRUE), can either vectors logicals layer (number hidden layers + 1 (final layer)) length one (default = TRUE). lambda numerical value indicating strength regularization: lambda penalty, \\(\\lambda * (L1 + L2)\\) (default = 0). alpha numerical vector add L1/L2 regularization training. Set alpha parameter layer \\((1 - \\alpha) * ||weights||_1 + \\alpha ||weights||^2\\). must fall 0 1 (default = 0.5). dropout numerical value dropout rate, probability node excluded training (default = 0). optimizer character value indicating optimizer use training network. user can specify: \"adam\" (ADAM algorithm), \"adagrad\" (adaptive gradient algorithm), \"rmsprop\" (root mean squared propagation), \"rprop” (resilient backpropagation), \"sgd\" (stochastic gradient descent). default, optimizer = “adam”. lr numerical value indicating learning rate given optimizer (default = 0.01). epochs numerical value indicating epochs training conducted (default = 100). device character value describing CPU/GPU device (\"cpu\", \"cuda\", \"mps\")  neural network trained (default = \"cpu\") early_stopping set integer, training terminate loss increases predetermined number consecutive epochs apply validation loss available. Default FALSE, early stopping applied. verbose verbose = TRUE, training curves DNN models displayed output, comparing training, validation baseline curves terms loss (y) number epochs (x) (default = TRUE). ... Currently ignored.","code":""},{"path":"/reference/SEMdnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"S3 object class \"DNN\" returned. list 5 objects: \"fit\", list DNN model objects, including: estimated covariance matrix (Sigma), estimated model errors (Psi), fitting indices (fitIdx), estimated connection weights (parameterEstimates), cowt=TRUE. “Yhat”, matrix prediction values sink mediator graph nodes. \"model\", list j=1,...,(L-1) fitted MLP network models. \"graph\", induced DAG input graph mapped data variables. cowt=TRUE, DAG colored based estimated connection weights, abs(W) > thr W < 0,  edge inhibited highlighted blue; otherwise, abs(W) > thr W > 0, edge activated highlighted red. \"data\", input training data subset mapping graph nodes.","code":""},{"path":"/reference/SEMdnn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"mapping data onto input graph, SEMdnn() creates set DNN models based topological layer (j=1,…,L) structure input graph. iteration, response (output) variables, y nodes j=1,...,(L-1) layer predictor (input) variables, x nodes belonging successive, (j+1),...,L layers. DNN model Multilayer Perceptron (MLP) network, every neuron node connected every neuron node hidden layer every hidden layer . neuron's value determined calculating weighted summation outputs hidden layer , applying activation function.  calculated value every neuron used input neurons layer , output layer reached.","code":""},{"path":"/reference/SEMdnn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"Amesöder, C., Hartig, F. Pichler, M. (2024), ‘cito': R package training neural networks using ‘torch'. Ecography, 2024: e07143. https://doi.org/10.1111/ecog.07143 Grassi M, Palluzzi F, Tarantino B (2022). SEMgraph: R Package Causal Network Analysis High-Throughput Data Structural Equation Models. Bioinformatics, 38 (20), 4829–4830 <https://doi.org/10.1093/bioinformatics/btac567>","code":""},{"path":"/reference/SEMdnn.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/SEMdnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Layer-wise SEM train with a Deep Neural Netwok (DNN) — SEMdnn","text":"","code":"# \\donttest{ if (torch::torch_is_installed()){  # load ALS data ig<- alsData$graph data<- alsData$exprs data<- transformData(data)$data group<- alsData$group  #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data)) #thr<- 1/sqrt(2*nrow(data[train, ])) #Joseph(2020, formula 22)  start<- Sys.time() dnn0 <- SEMdnn(ig, data, train, cowt = TRUE, thr = NULL,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE) end<- Sys.time() print(end-start)  #str(dnn0, max.level=2) dnn0$fit$fitIdx dnn0$fit$parameterEstimates gplot(dnn0$graph) table(E(dnn0$graph)$color)  #...with a binary outcome (1=case, 0=control)  ig1<- mapGraph(ig, type=\"outcome\"); gplot(ig1) outcome<- ifelse(group == 0, -1, 1); table(outcome) data1<- cbind(outcome, data); data1[1:5,1:5]  start<- Sys.time() dnn1 <- SEMdnn(ig1, data1, train, cowt = TRUE, thr = NULL,       #loss = \"mse\", hidden = 5*K, link = \"selu\",       loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE) end<- Sys.time() print(end-start)  #str(dnn1, max.level=2) dnn1$fit$fitIdx dnn1$fit$parameterEstimates gplot(dnn1$graph)  table(E(dnn1$graph)$color)  #...with input -> hidden structure -> output : # source nodes -> graph layer structure -> sink nodes  #View topological layer (TL) ordering dag<- graph2dag(dnn0$graph, data[train,]) L<- SEMdeep:::buildLevels(dag);L K<- unlist(lapply(L, length));K K<- rev(K[-c(1,length(K))]);K  ig2<- mapGraph(ig, type=\"source\"); gplot(ig2)  start<- Sys.time() dnn2 <- SEMdnn(ig2, data, train, cowt = TRUE, thr = NULL,       loss = \"mse\", hidden = 5*K, link = \"selu\",       #loss = \"mse\", hidden = c(10, 10, 10), link = \"selu\",       validation = 0, bias = TRUE, lr = 0.01,       epochs = 32, device = \"cpu\", verbose = TRUE) end<- Sys.time() print(end-start)  #Visualization of the neural network structure nplot(dnn2$model[[1]], bias=FALSE)  #str(dnn2, max.level=2) dnn2$fit$fitIdx mean(dnn2$fit$Psi) dnn2$fit$parameterEstimates gplot(dnn2$graph) table(E(dnn2$graph)$color) } #> Conducting the nonparanormal transformation via shrunkun ECDF...done. #> 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch   train_l valid_l #> 32    32 0.2289411      NA #>  #> 2 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.2606372      NA #>  #> 3 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.2412261      NA #>  #> 4 : z596 z4217   #>    epoch   train_l valid_l #> 32    32 0.3060135      NA #>  #> 5 : z1616   #>    epoch   train_l valid_l #> 32    32 0.2760505      NA #>  #> DNN solver ended normally after 736 iterations  #>  #>  logL: -32.35825  srmr: 0.0861234  #>  #> Time difference of 5.444144 secs   #> 1 : zoutcome   #>    epoch     train_l valid_l #> 32    32 0.007521793      NA #>  #> 2 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch   train_l valid_l #> 32    32 0.2159616      NA #>  #> 3 : z842 z1432 z5600 z5603 z6300   #>    epoch   train_l valid_l #> 32    32 0.2586219      NA #>  #> 4 : z54205 z5606 z5608   #>    epoch   train_l valid_l #> 32    32 0.2265621      NA #>  #> 5 : z596 z4217   #>    epoch  train_l valid_l #> 32    32 0.292662      NA #>  #> 6 : z1616   #>    epoch   train_l valid_l #> 32    32 0.2821012      NA #>  #> DNN solver ended normally after 768 iterations  #>  #>  logL: -32.18342  srmr: 0.0864829  #>  #> Time difference of 6.824774 secs   #> 1 : z10452 z84134 z836 z4747 z4741 z4744 z79139 z5530 z5532 z5533 z5534 z5535   #>    epoch   train_l valid_l #> 32    32 0.4537093      NA #>  #> DNN solver ended normally after 384 iterations  #>  #>  logL: -20.91299  srmr: 0.1696513  #>  #> Time difference of 1.430519 secs   #>  #>     gray50       red2 royalblue3  #>         46         12         14  # }"},{"path":"/reference/SEMml.html","id":null,"dir":"Reference","previous_headings":"","what":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"function converts graph collection nodewise-based models: mediator sink variable can expressed function parents. Based assumed type relationship, .e. linear non-linear, SEMml() fits ML model node (variable) non-zero incoming connectivity. model fitting repeated equation-equation (r=1,...,R) times, R number mediators sink nodes.","code":""},{"path":"/reference/SEMml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"","code":"SEMml(   graph,   data,   train = NULL,   algo = \"sem\",   vimp = FALSE,   thr = NULL,   verbose = FALSE,   ... )"},{"path":"/reference/SEMml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"graph igraph object. data matrix rows corresponding subjects, columns graph nodes (variables). train numeric vector specifying row indices corresponding train dataset (default = NULL). algo ML method used nodewise-network predictions. Six algorithms can specified: algo=\"sem\" (default) linear SEM, see SEMrun. algo=\"gam\" generalized additive model, see gam. algo=\"rf\" random forest model, see ranger. algo=\"xgb\" XGBoost model, see xgboost. algo=\"nn\" small neural network model (1 hidden layer 10 nodes), see nnet. algo=\"dnn\" large neural network model (1 hidden layers 1000 nodes), see dnn. vimp Logical value(default=FALSE). TRUE compute variable importance, considering: () squared value t-statistic F-statistic model parameters \"sem\" \"gam\"; (ii) variable importance importance xgb.importance functions \"rf\" \"xgb\"; (iii) Olden's connection weights \"nn\" \"dnn\". thr numerical value indicating threshold apply variable importance color graph. thr=NULL (default), threshold set thr = abs(mean(vimp)). verbose logical value. FALSE (default), processed graph plotted screen. ... Currently ignored.","code":""},{"path":"/reference/SEMml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"S3 object class \"ML\" returned. list 5 objects: \"fit\", list ML model objects, including: estimated covariance matrix (Sigma),  estimated model errors (Psi), fitting indices (fitIdx), signed Shapley R2 values (parameterEstimates), shap = TRUE, \"Yhat\", matrix predictions sink mediator graph nodes. \"model\", list fitted nodewise-based models (sem, gam, rf, xgb nn). \"graph\", induced DAG input graph  mapped data variables. vimp = TRUE, DAG colored based variable importance measure, .e., abs(vimp) > thr highlighted red (vimp > 0) blue (vimp < 0). \"data\", input training data subset mapping graph nodes.","code":""},{"path":"/reference/SEMml.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"mapping data onto input graph, SEMml() creates set nodewise-based models based directed links, .e., set edges pointing direction, two nodes input graph causally relevant . mediator sink variables can characterized detail functions parents. ML model (sem, gam, rf, xgb, nn, dnn) can fitted variable non-zero inbound connectivity, taking account kind relationship (linear non-linear). R representing number mediators sink nodes network, model fitting process performed equation--equation (r=1,...,R) times.","code":""},{"path":"/reference/SEMml.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"Grassi M, Palluzzi F, Tarantino B (2022). SEMgraph: R Package Causal Network Analysis High-Throughput Data Structural Equation Models. Bioinformatics, 38 (20), 4829–4830 <https://doi.org/10.1093/bioinformatics/btac567> Hastie, T. Tibshirani, R. (1990) Generalized Additive Models. London: Chapman Hall. Breiman, L. (2001), Random Forests, Machine Learning 45(1), 5-32. Chen, T., & Guestrin, C. (2016). XGBoost: Scalable Tree Boosting System. Proceedings 22nd ACM SIGKDD International Conference Knowledge Discovery Data Mining. Ripley, B. D. (1996) Pattern Recognition Neural Networks. Cambridge. Redell, N. (2019). Shapley Decomposition R-Squared Machine Learning Models. arXiv: Methodology.","code":""},{"path":"/reference/SEMml.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"Mario Grassi mario.grassi@unipv.","code":""},{"path":"/reference/SEMml.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nodewise-predictive SEM train using Machine Learning (ML) — SEMml","text":"","code":"# \\donttest{ # Load Amyotrophic Lateral Sclerosis (ALS) data<- alsData$exprs; dim(data) #> [1] 160 318 data<- transformData(data)$data #> Conducting the nonparanormal transformation via shrunkun ECDF...done. group<- alsData$group; table (group) #> group #>   0   1  #>  21 139  ig<- alsData$graph; gplot(ig)   #...with train-test (0.5-0.5) samples set.seed(123) train<- sample(1:nrow(data), 0.5*nrow(data))  start<- Sys.time() # ... rf #res1<- SEMml(ig, data, train, algo=\"rf\", vimp=FALSE) res1<- SEMml(ig, data, train, algo=\"rf\", vimp=TRUE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  RF solver ended normally after 23 iterations  #>  #>  logL: -33.22824  srmr: 0.0859545  #>   # ... xgb #res2<- SEMml(ig, data, train, algo=\"xgb\", vimp=FALSE) res2<- SEMml(ig, data, train, algo=\"xgb\", vimp=TRUE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  XGB solver ended normally after 23 iterations  #>  #>  logL: 70.10035  srmr: 0.0014393  #>   # ... nn #res3<- SEMml(ig, data, train, algo=\"nn\", vimp=FALSE) res3<- SEMml(ig, data, train, algo=\"nn\", vimp=TRUE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  NN solver ended normally after 23 iterations  #>  #>  logL: -37.48083  srmr: 0.1987503  #>   # ... gam #res4<- SEMml(ig, data, train, algo=\"gam\", vimp=FALSE) res4<- SEMml(ig, data, train, algo=\"gam\", vimp=TRUE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  GAM solver ended normally after 23 iterations  #>  #>  logL: -46.77283  srmr: 0.3819281  #>  end<- Sys.time() print(end-start) #> Time difference of 4.349824 secs  # ... sem #res5<- SEMml(ig, data, train, algo=\"sem\", vimp=FALSE) res5<- SEMml(ig, data, train, algo=\"sem\", vimp=TRUE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #>  #>  SEM solver ended normally after 23 iterations  #>  #>  logL: -47.90891  srmr: 0.3040025  #>   #str(res5, max.level=2) res5$fit$fitIdx #>        logL        amse        rmse   srmr.srmr  #> -47.9089078   0.8166851   0.9037063   0.3040025  res5$fit$parameterEstimates #>          lhs op   rhs varImp #> 1      10452  ~  6647  0.164 #> z5606   1432  ~  5606  9.823 #> z5608   1432  ~  5608 29.897 #> z7132   1616  ~  7132  2.654 #> z7133   1616  ~  7133  0.132 #> 11      4217  ~  1616  2.781 #> z1432   4741  ~  1432  7.768 #> z5600   4741  ~  5600  0.554 #> z5603   4741  ~  5603  0.745 #> z6300   4741  ~  6300  0.801 #> z5630   4741  ~  5630  3.339 #> z14321  4744  ~  1432  5.927 #> z56001  4744  ~  5600  0.563 #> z56031  4744  ~  5603  0.351 #> z63001  4744  ~  6300  0.347 #> z56301  4744  ~  5630  4.134 #> z6647   4747  ~  6647 25.892 #> z14322  4747  ~  1432  8.210 #> z56002  4747  ~  5600  0.050 #> z56032  4747  ~  5603  4.131 #> z63002  4747  ~  6300  0.973 #> z56302  4747  ~  5630  1.506 #> z581   54205  ~   581 22.353 #> z572   54205  ~   572  0.029 #> z596   54205  ~   596  0.080 #> z598   54205  ~   598  2.735 #> 12      5530  ~  6647  9.332 #> 13      5532  ~  6647 17.839 #> 14      5533  ~  6647  6.467 #> 15      5534  ~  6647  9.491 #> 16      5535  ~  6647 16.447 #> z56061  5600  ~  5606  2.159 #> z56081  5600  ~  5608  5.425 #> z56062  5603  ~  5606  4.948 #> z56082  5603  ~  5608  0.037 #> 17      5606  ~  4217  0.669 #> 18      5608  ~  4217 34.179 #> 19       596  ~  6647  0.416 #> z56063  6300  ~  5606  1.325 #> z56083  6300  ~  5608 14.431 #> 110    79139  ~  6647 19.233 #> 111      836  ~   842 22.853 #> 112    84134  ~  6647  7.515 #> z54205   842  ~ 54205 41.336 #> z317     842  ~   317  6.901 gplot(res5$graph)   #Comparison of AMSE (in train data) rf <- res1$fit$fitIdx[2];rf #>      amse  #> 0.2327284  xgb<- res2$fit$fitIdx[2];xgb #>         amse  #> 0.0001865892  nn <- res3$fit$fitIdx[2];nn #>      amse  #> 0.4966564  gam<- res4$fit$fitIdx[2];gam #>      amse  #> 0.7525285  sem<- res5$fit$fitIdx[2];sem #>      amse  #> 0.8166851   #Comparison of SRMR (in train data) rf <- res1$fit$fitIdx[4];rf #>       srmr  #> 0.08595454  xgb<- res2$fit$fitIdx[4];xgb #>        srmr  #> 0.001439282  nn <- res3$fit$fitIdx[4];nn #>      srmr  #> 0.1987503  gam<- res4$fit$fitIdx[4];gam #>      srmr  #> 0.3819281  sem<- res5$fit$fitIdx[4];sem #> srmr.srmr  #> 0.3040025   #Comparison of VIMP (in train data) table(E(res1$graph)$color) #rf #>  #> gray50   red2  #>     28     17  table(E(res2$graph)$color) #xgb #>  #> gray50   red2  #>     26     19  table(E(res3$graph)$color) #nn #>  #>     gray50       red2 royalblue3  #>         31          4         10  table(E(res4$graph)$color) #gam #>  #> gray50   red2  #>     31     14  table(E(res5$graph)$color) #sem #>  #> gray50   red2  #>     31     14   #Comparison of AMSE (in test data) print(predict(res1, data[-train, ])$PE[1]) #rf #>     amse  #> 1.108783  print(predict(res2, data[-train, ])$PE[1]) #xgb #>     amse  #> 1.548598  print(predict(res3, data[-train, ])$PE[1]) #nn #>     amse  #> 1.643303  print(predict(res4, data[-train, ])$PE[1]) #gam #>     amse  #> 0.912187  print(predict(res5, data[-train, ])$PE[1]) #sem #>      amse  #> 0.8973153   #...with a binary outcome (1=case, 0=control)  ig1<- mapGraph(ig, type=\"outcome\"); gplot(ig1)  outcome<- ifelse(group == 0, -1, 1); table(outcome) #> outcome #>  -1   1  #>  21 139  data1<- cbind(outcome, data); data1[1:5,1:5] #>      outcome        207         208      10000       284 #> ALS2       1 -1.8273895 -0.45307006 -0.1360061 0.4530701 #> ALS3       1 -2.5616910 -0.96201413  0.3160400 0.6762093 #> ALS4       1 -0.8003346  0.82216031 -1.1521227 0.5613048 #> ALS5       1 -2.1342965 -0.98709115  1.1521227 0.5064807 #> ALS6       1 -2.0111279  0.02393297  0.5987578 0.1360061  res6 <- SEMml(ig1, data1, train, algo=\"nn\", vimp=TRUE) #> 1 : z10452  #> 2 : z1432  #> 3 : z1616  #> 4 : z4217  #> 5 : z4741  #> 6 : z4744  #> 7 : z4747  #> 8 : z54205  #> 9 : z5530  #> 10 : z5532  #> 11 : z5533  #> 12 : z5534  #> 13 : z5535  #> 14 : z5600  #> 15 : z5603  #> 16 : z5606  #> 17 : z5608  #> 18 : z596  #> 19 : z6300  #> 20 : z79139  #> 21 : z836  #> 22 : z84134  #> 23 : z842  #> 24 : zoutcome  #>  #>  NN solver ended normally after 24 iterations  #>  #>  logL: -35.20595  srmr: 0.2007807  #>  gplot(res6$graph)  table(E(res6$graph)$color) #>  #>     gray50       red2 royalblue3  #>         43          7          7   mse6 <- predict(res6, data1[-train, ]) yobs <- group[-train] yhat <- mse6$Yhat[ ,\"outcome\"] performance(yobs, yhat, thr=0, F1=TRUE) #>     ypred #> yobs  0  1 #>    0  4  2 #>    1 18 56 #>  #>         pre       rec        f1       mcc #> 1 0.9655172 0.7567568 0.8484848 0.2497704 performance(yobs, yhat, thr=0, F1=FALSE) #>     ypred #> yobs  0  1 #>    0  4  2 #>    1 18 56 #>  #>          sp        se  acc       mcc #> 1 0.6666667 0.7567568 0.75 0.2497704 # }"},{"path":"/news/index.html","id":"version-010-release-notes","dir":"Changelog","previous_headings":"","what":"Version 0.1.0 Release Notes","title":"Version 0.1.0 Release Notes","text":"First stable version CRAN","code":""}]
